You can check your quota and allocations on LUMI using the lumi-workspaces 
command that is available without loading any additional module.

=====

The preferred way to use compilers on a Cray system is through one of the
PrgEnv modules: PrgEnv-gnu for the GNU compilers, PrgEnv-cray for the Cray
compilers, PrgEnv-aocc for the AMD AOCC CPU-only compilers and PrgEnv-amd
for the ROCm-based AMD compilers for GPU. The compilers are then called
through wrapper scripts: cc for the C compiler, CC for the C++ compiler and 
ftn for the Fortran compiler.

However, when using the LUMI software stacks, it is preferable to load
cpeGNU, cpeCray, cpeAOCC or cpeAMD instead of PrgEnv-gnu, PrgEnv-cray,
PrgEnv-aocc and PrgEnv-amd respectively, but you should still use the 
wrappers. This ensures optimal compatibility with software installed in
the LUMI stacks.

=====

There is no need to specify a target with the compiler wrappers as they
determine the target from the loaded craype-x86-* module. Compiling for MPI
is automatically enabled when a cray-mpich module is loaded, and the Cray
LibSci library (which contains BLAS, LAPACK and ScaLAPACK) is automatically
linked when the cray-libsci module is loaded.

=====

LUMI has no mpirun or mpiexec command. MPI programs using Cray MPICH are 
started using the Slurm srun command.

=====

The preferred way to contact the LUMI User Support Team is using the forms on
https://lumi-supercomputer.eu/user-support/need-help/. Please don't recycle an
old ticket for a new request but fill in the form again. This ensures the
proper handling of your request. LUMI user support is active on workdays
from 8am till 6 pm central-European time so covers the regular 9-to-5 business
hours in allmost all countries of EuroHPC.

=====

LUMI is an abbreviation of Large Unified Modern Infrastructure. The LUMI
consortium countries are Finland, Belgium, the Czech Republic, Denmark, Estonia,
Iceland, Norway, Poland, Sweden, Switzerland and The Netherlands.

LUMI means snow in Finnish. Snow has also inspired the look of the LUMI data
center.

=====

LUMI uses warm-water cooling, which enables its waste heat to be utilized in the
district heating network of the city of Kajaani, and thus replaces heat produced
by fossil fuels.

The waste heat from LUMI that can be used in Kajaani’s district heating network
is equivalent to up to 20 per cent of the energy that Kajaani needs to use in
the area’s district heating. This reuse of waste heat will reduce the annual
carbon dioxide footprint of Kajaani by 13,500 tons – an amount that equals the
output from 4000 passenger cars.

=====

With software built with EasyBuild (as is most software in the LUMI stack),
it is very easy to find the location of the software binaries. After loading
the module, an environment variable based on the name of the module will be
defined. E.g., after loading one of the Boost modules, the location of
Boost will be given by the EBROOTBOOST environment variable. Try
  env | egrep ^EBROOT
in a shell to see all EBROOT variables that are defined after loading some
modules.

=====

The behaviour of many modules of the Cray Programming Environment depends
on the target modules that are loaded. All compilers on LUMI except for the
gcc implementation provided by the operating system support proper 
optimizations for the zen2 and zen3 processors. The zen3 architectures is 
the core architecture of the AMD Epyc Milan CPUs in the LUMI-C compute nodes 
and the AMD EPYC Trento CPUs in the LUMI-G compute nodes. You may get get a 
little extra performance by optimising for zen3 specifically. The CrayEnv 
environment will automatically load the best target CPU, network and a
ccelerator target modules for each node. You can always restore them by 
reloading the CrayEnv module (and there is no need to unload first).

In a job script, the environment is copied from the login nodes, so you'll
still have the Rome target modules. Here also reloading the CrayEnv module
will set the ones for the compute node you're running on.

=====

Switching versions of the Cray Programming Environment should be possible by
loading the cpe module for the version. However, it is a fragile process and
often produces error messages. One clean way to do it, is in the CrayEnv
environment: Clean the environment using "module purge", load the cpe/yy.mm
module of your choice, and then load the PrgEnv module of your choice in its
default version. You have to use different module load statements though
for the cpe and PrgEnv module as the cpe module only takes full effect at the
next module load command. E.g., to load the 23.09 version of PrgEnv-cray:
$ ml CrayEnv
$ module purge
$ module load cpe/23.09
$ module load PrgEnv-cray

=====

The interconnect on LUMI does not support UCX. 

For technical people: Only libfabric with the so-called cassini(CXI) provider 
are supported for high performance communication.

====

When launching jobs, you need to use your LUMI project number for the account
information, and not a program number that may have been assigned to your
project by the instance from which you requested your allocation.

This LUMI project number actually also corresponds to the project group
assigned to your project. It is easy to see to which project(s) you belong
by running the groups command or the lumi-workspaces command or the
lumi-ldap-userinfo command on the command line.

====

On LUMI, it is not possible to use ssh to reach the compute nodes. All 
connections to the compute nodes have to be made through the resource 
manager, so you have to use srun instead. See also
https://docs.lumi-supercomputer.eu/runjobs/scheduled-jobs/interactive/

====

Sharing accounts is strictly forbidden on LUMI. When discovered, you will be 
banned from the system. This is not only about sharing with physical people,
but it is also forbidden to give third-party services access to your account or
software running under your account, which is, e.g., the case when you use
VSCode remote tunnels rather than the VSCode Remote ssh plugin, or when you
use Ngrok. This list is not exhaustive. There are many other such services,
e.g., some use cases of cloudflare or Gradio.

Also forbidden is to run a server on LUMI and make that server accessible 
to others than yourself, e.g., through an ssh tunnel from a publicly accessible
machine, as that effectively allows others to trigger the execution of code
on LUMI and hence is also a way of sharing accounts.

Internet-accessible services are typically run in a fully isolated environment
and often in a so-called "demilitarized zone" from which it is extremely hard
to get access to other computers at that site. This is exactly the opposite of
what a supercomputer is, as that is a "shared nearly everything" setup to enable
users to run large parallel jobs easily and efficiently, so opening up your
account to the outside world poses a high security risk, not only to you, but
also to other users of LUMI. LUMI is not a cloud infrastructure where virtual
machines and/or containers offer protection to other users and the system
itself.
