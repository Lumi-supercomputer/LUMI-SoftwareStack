You can check your quota on LUMI using the lumi-workspaces command that you
can find in the lumi-workspaces module in the LUMI and CrayEnv environments.

=====

The preferred way to use compilers on a Cray system is through one of the
PrgEnv modules: PrgEnv-gnu for the GNU compilers, PrgEnv-cray for the Cray
compilers, PrgEnv-aocc for the AMD AOCC CPU-only compilers and PrgEnv-amd
for the ROCm-based AMD compilers for GPU. The compilers are then called
through wrapper scripts: cc for the C compiler, CC for the C++ compiler and 
ftn for the Fortran compiler.

However, when using the LUMI software stacks, it is preferable to load
cpeGNU, cpeCray, cpeAOCC or cpeAMD instead of PrgEnv-gnu, PrgEnv-cray,
PrgEnv-aocc and PrgEnv-amd respectively, but you should still use the 
wrappers. This ensures optimal compatibility with software installed in
the LUMI stacks.

=====

There is no need to specify a target with the compiler wrappers as they
determine the target from the loaded craype-x86-* module. Compiling for MPI
is automatically enabled when a cray-mpich module is loaded, and the Cray
LibSci library (which contains BLAS, LAPACK and ScaLAPACK) is automatically
linked when the cray-libsci module is loaded.

=====

LUMI has no mpirun or mpiexec command. MPI programs are started using the
Slurm srun command.

=====

The preferred way to contact the LUMI User Support Team is using the forms on
https://lumi-supercomputer.eu/user-support/need-help/. Please don't recycle an
old ticket for a new request but fill in the form again. This ensures the
proper handling of your request. LUMI user support is active on workdays
from 8am till 6 pm central-European time so covers the regular 9-to-5 business
hours in all countries of EuroHPC.

=====

LUMI is an abbreviation of Large Unified Modern Infrastructure. The LUMI
consortium countries are Finland, Belgium, the Czech Republic, Denmark, Estonia,
Iceland, Norway, Poland, Sweden, and Switzerland.

LUMI means snow in Finnish. Snow has also inspired the look of the LUMI data
center.

=====

LUMI uses warm-water cooling, which enables its waste heat to be utilized in the
district heating network of the city of Kajaani, and thus replaces heat produced
by fossil fuels.

The waste heat from LUMI that can be used in Kajaani’s district heating network
is equivalent to up to 20 per cent of the energy that Kajaani needs to use in
the area’s district heating. This reuse of waste heat will reduce the annual
carbon dioxide footprint of Kajaani by 13,500 tons – an amount that equals the
output from 4000 passenger cars.

=====

The CrayEnv stack is our solution to use the Cray Programming Environment fully
as intended by Cray. However, when using the LUMI software stack, it is better to
use the cpeGNU or cpeCray modules to load the GNU respectively the Cray compilers
in your environment as that guarantees a setup that is compatible with the
cpeGNU and cpeCray modules respectively. You're free to do otherwise if you're
an experienced user, but you have to realise that not all combinations of
compiler and software modules will work.

=====

With software built with EasyBuild (as is most software in the LUMI stack),
it is very easy to find the location of the software binaries. After loading
the module, an environment variable based on the name of the module will be
defined. E.g., after loading one of the Boost modules, the location of
Boost will be given by the EBROOTBOOST environment variable. Try
  env | egrep ^EBROOT
in a shell to see all EBROOT variables that are defined after loading some
modules.

=====

The behaviour of many modules of the Cray Programming Environment depends
on the target modules that are loaded. Compilers prior to the 21.11 environment
did not optimize specifically for zen3, the core architecture of the AMD
Epyc Milan CPUs in the LUMI-C compute nodes and the AMD Epyc Trento CPUs
in the LUMI-G compute nodes. However, gcc 11 and later and also the Cray
compilers included from the 21.11 environment onwards can get a little extra
performance by optimising for zen3 specifically. The CrayEnv environment
will automatically load the best target CPU, network and accelerator target
modules for each node. You can always restore them by reloading the
CrayEnv module (and there is no need to unload first).

In a job script, the environment is copied from the login nodes, so you'll
still have the Rome target modules. Here also reloading the CrayEnv module
will set the ones for the compute node you're running on.

=====

Switching versions of the Cray Programming Environment should be possible by
loading the cpe module for the version. However, it is a fragile process and
often produces error messages. One clean way to do it, is in the CrayEnv
environment: Clean the environment using "module purge", load the cpe/yy.mm
module of your choice, and then load the PrgEnv module of your choice in its
default version. You have to use different module load statements though
for the cpe and PrgEnv module as the cpe module only takes full effect at the
next module load command. E.g., to load the 22.08 version of PrgEnv-cray:
$ ml CrayEnv
$ module purge
$ module load cpe/22.08
$ module load PrgEnv-cray

=====

The interconnect on LUMI after the update of late June and early July 2022
does not support UCX, so craype-network-ucx and cray-mpich-ucx no longer
work as before or will fall back to (slow) TCP communication.

For technical people: only libfabric with the so-called cassini provider are
supported for high performance communication.

====

When launching jobs, you need to use your LUMI project number for the account
information, and not a program number that may have been assigned to your
project by the instance from which you requested your allocation.

This LUMI project number actually also corresponds to the project group
assigned to your project. It is easy to see to which project(s) you belong
by running the groups command on the command line.
