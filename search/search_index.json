{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Docs for the LUMI software stack","text":"<ul> <li> <p>What's new or different?</p> </li> <li> <p>Installation instructions for the pilot phase</p> </li> <li> <p>Setup of the LUMI module system</p> </li> <li>EasyBuild setup</li> <li>Cray PE integration</li> <li>Some procedures</li> <li>The directpry structure in the installation directory</li> <li>An overview of files in several of the repository directories and where they     are used</li> <li> <p>The contents of the SitePackage.lua file</p> </li> <li> <p>Toolchain documentation</p> <ul> <li>EasyBuild toolchains common options</li> <li>Common information to all cpe* toolchains</li> <li>cpeCray toolchain</li> <li>cpeGNU toolchain</li> <li>cpeAOCC toolchain</li> <li>cpeAMD toolchain</li> <li>Current configurations on LUMI</li> </ul> </li> <li> <p>Documentation on our EasyBlocks:</p> <ul> <li>The CrayPEToolchain EasyBlock</li> </ul> </li> <li> <p>A number of failed experiments during the development, to avoid making the     same mistake twice</p> </li> </ul>"},{"location":"CrayPEToolchain/","title":"The CrayPEToolchain EasyBlock and cpeCray/cpeGNU/cpeAMD modules","text":""},{"location":"CrayPEToolchain/#introduction","title":"Introduction","text":"<p>Our CrayPEToolchain EasyBlock allows for many different scenarios to generate the cpeCray/cpeGNU/cpeAMD modules:</p> <ul> <li> <p>The <code>cpe</code> module can be loaded first, last or not at all. Note that if the     module is not loaded at all, it may be wise to have a different way of setting     the default versions for the Cray PE modules.</p> <p>On LUMI, in the LUMI software stacks, these default versions are already set by the <code>LUMIstack_&lt;yy.mm&gt;_modulerc.lua</code> files.</p> <p>In version 21.04 of the CPE, the <code>cpe</code> modules still have several problems, partly due to an LMOD restriction and partly due to bugs in those modules:</p> <ul> <li> <p>The <code>cpe</code> modules set <code>LMOD_MODULERCFILE</code> through <code>setenv</code> rather than     <code>prepend_path</code> or <code>append_path</code> so they overwrite any file that sets     system-wide defaults and visibility from other sources, which is not desirable.</p> </li> <li> <p>A change to <code>LMOD_MODULERCFILE</code> has only effect the next time a module command     is executed. This is a restriction not only of LMOD version <code>8.3.x</code> used in the     21.04 CPE, but also of versions in the 8.4 and 8.5 series. Hence loading the     <code>cpe/yy.mm</code> module before loading other versionless modules for the CPE components     will not have the desired effect of loading the versions for that specific version of     the CPE.</p> </li> <li> <p>The <code>cpe</code> modules do contain code to reload already loaded modules from the CPE in     the correct version, but that code is also broken in the <code>21.04</code> version as the     modules may be loaded in an order in which a module that has already been reloaded     in the correct version, gets reloaded once more with a versionless load, which may     reload the wrong version. This is because the LUA loop with <code>pairs</code> doesn't have     a fixed order of going over the entries in the LUA table. The order should be such     that no module reloads an other module that has already been reloaded in the correct     version.</p> </li> </ul> </li> <li> <p>The matching <code>PrgEnv-*</code> module can either be loaded, or its loading can just be emulated     by only setting the environment variable that this module sets, but relying on the <code>cray_targets</code>     variable and dependencies list to load all Cray PE components.</p> <p>The reason to avoid loading the <code>PrgEnv-*</code> module is reproducibility. That module depends on a file in <code>/etc</code> to define the components that will be loaded, and that file cannot distinguish between versions of the CPE. Hence if changes to that file would be made, it has an effect on the working of all <code>cpe*</code> modules that EasyBuild may already have generated.</p> </li> <li> <p>It is possible to specify target modules via <code>cray_targets</code>.  This is a list just as the     dependencies. They will be loaded after the <code>PrgEnv-*</code> module (if the latter is loaded) but     before other dependencies specified by <code>dependencies</code>. They do not need to be defined in     the EasyBuild external modules file. We chose to load them after the <code>PrgEnv-*</code>     module (if the latter is loaded) to be able to overwrite Cray targeting modules     loaded by the latter.</p> </li> <li> <p>Dependencies in this case will be external modules. It is possible to specify versions by using,     e.g.,     <pre><code>( 'gcc/9.3.0', EXTERNAL_MODULE)\n</code></pre>     Versions should be specified if the <code>cpe</code> module is not loaded, even on LUMI, as if     a user would execute     <pre><code>module load LUMI/21.04 cpeGNU/21.04\n</code></pre>     the wrong versions of CPE components might be loaded because of the same LMOD restriction     that causes the problems with the Cray <code>cpe/yy.mm</code> modules: The <code>LUMI/yy.mm</code>     module will add a file that sets the default versions of CPE compoments for the     requested LUMI software stack and matching CPE version, but those changes only     take effect at the following <code>module</code> command, so the <code>cpeGNU/21.04</code> module     which is loaded in the above example will not yet see the correct default versions     of the modules.</p> <p>Note also that if versions are specified but the <code>cpe</code> module is loaded at the end, modules might be reloaded in a different version.</p> </li> <li> <p>The default value for various parameters is chosen to generate module files that     are as similar as possible to those used ast CSCS (or at least those used for their     20.04 environment), but are not the defaults initially used on LUMI.</p> </li> </ul>"},{"location":"CrayPEToolchain/#supported-extra-parameters-for-the-easyconfig-files","title":"Supported extra parameters for the EasyConfig files","text":"<p>The <code>CrayPEToolchain</code> EasyBlock supports the following parameters:</p> <ul> <li> <p><code>PrgEnv</code>: Sets the <code>PrgEnv-*</code> module to load or emulate.</p> <ul> <li>The default is to derive the value from the name of the module to generate:<ul> <li><code>PrgEnv = 'cray'</code> for <code>cpeCray</code></li> <li><code>PrgEnv = 'gnu'</code> for <code>cpeGNU</code></li> <li><code>PrgEnv = 'aocc'</code> for <code>cpeAMD</code></li> <li><code>PrgEnv = 'intel'</code> for <code>cpeIntel</code></li> <li><code>PrgEnv = 'nvidia'</code> for <code>cpeNVIDIA</code> (not tested as we have no access to     a machine with a fully working version of this environment)</li> </ul> </li> <li>It is also possible to specify any of these values, or even a different value for a     <code>PrgEnv-*</code> module that is not yet recognized by the EasyBlock.</li> </ul> </li> <li> <p><code>PrgEnv_load</code>: Boolean value, indicate if the <code>PrgEnv</code> module should be loaded     explicitly (if True) or not (if False).</p> <p>Default is <code>True</code>.</p> <p>If you want to hard-code a version, you can do so by specifying the module with the version in the dependencies.</p> <p>It is important that all <code>cpe*</code> modules available in the system at the same time are also generated with the same setting for <code>PrgEnv_load</code> as otherwise the conflict resolution between those modules would not work correctly.</p> </li> <li> <p><code>PrgEnv_family</code>:</p> <ul> <li> <p>If <code>cpeToolchain</code>, the module will declare itself a member of the <code>cpeToolchain</code>     family. If all <code>cpe*</code> modules are generated that way, this will ensure that no two     different <code>cpe*</code> modules will be loaded simulataneously, which wouldn't work correctly     anyway with the Cray compiler wrappers.</p> <p>If <code>PrgEnv_load</code> is false, it will also force unload all <code>PrgEnv-*</code> modules to ensure that none is loaded. Otherwise it relies on the  family-mechanism used in the LMOD <code>PrgEnv-*</code> modules to do the job.</p> <p>This is the most robust option when explicitly loading a <code>PrgEnv-*</code> module and using LMOD as LMOD will then ensure that no two <code>cpe*</code> modules will be loaded simultaneously and the family mechanism used in the Cray <code>PrgEnv-*</code> modules will do the same for those modules.</p> </li> <li> <p>If <code>PrgEnv</code>, the module will declare itself a member of the <code>PrgEnv-</code>     family. This will generate an error if <code>PrgEnv_load</code> is True as one     cannot load two modules of the same family but is the most robust ootion when     using LMOD and emulating the <code>PrgEnv-*</code> module.</p> <p>The LMOD family feature will take care of unloading all other <code>PrgEnv-*</code> or <code>cpe*</code> modules as they would conflict with the current module.</p> </li> <li> <p>If <code>None</code> (default), which is the only setting that works when TCL-based modules are     used and is therefore the default, the module will start with unload commands     for all known <code>PrgEnv-*</code> and all <code>cpe*</code> modules except itself and the <code>PrgEnv-*</code>     module that it uses (if it uses one).</p> </li> </ul> <p>It is important that all <code>cpe*</code> modules available in the system at the same time are also generated with the same setting for <code>PrgEnv_family</code> as otherwise the conflict resolution between those modules would not work correctly.</p> </li> <li> <p><code>CPE_compiler</code> specifies the (versionless) compiler to load. Possible values are:</p> <ul> <li> <p><code>None</code> (default): Derive the name of the compiler module from the name of the     module to generate. This may not yet work for <code>cpeNVIDIA</code> as it is not clear     what the name of the compiler module will be.</p> <p>If will not add an additional load if that compiler module is already specified in the dependencies.</p> <p>Note that this will load the module without specifying the version, so it only makes sense to rely on the autodetect feature if the <code>cpe</code> module is loaded (and if the bugs with that one are fixed).</p> </li> <li> <p>Any other value will be considered the name of the compiler module to load.     The module should be versionless. If you want to specify a version, you can     do so via <code>dependencies</code>.</p> <p>No separate load will be generated if the compiler module is also found in the list of dependencies.</p> </li> </ul> </li> <li> <p><code>CPE_version</code>: Version of the cpe module to use (if it is used). Possible values:</p> <ul> <li> <p><code>None</code> (default): Determine the version from the version of the module to generate,     i.e., the <code>version</code> parameter in the EasyConfig.</p> </li> <li> <p>Any other value is interpreted as the value to load.</p> </li> </ul> </li> <li> <p><code>CPE_load</code>:  Possible values:</p> <ul> <li> <p><code>first</code> (default): Load as the very first module. This does not make sense until the LMOD     problems with <code>LMOD_MODULERCFILE</code> are fixed.</p> </li> <li> <p><code>after</code>: Load immediately after loading <code>PrgEnv-*</code> but before loading any     other module. This does not make too much sense until the LMOD problems with     <code>LMOD_MODULERCFILE</code> are fixed, but it could be a way to first load modules     the Cray way and then correct by manually loading correct versions via the     <code>cray_targets</code> and <code>dependencies</code> parameters.</p> <p>This value will produce an error message when <code>PrgEnv_load</code> is set to <code>False</code>.</p> </li> <li> <p><code>last</code>: Load as the last module. Currently this does not make sense until     the problems with the <code>cpe</code> module are fully fixed, and on LUMI, until the problem     with overwriting <code>LMOD_MODIULERCFILE</code> is fixed.</p> </li> <li> <p><code>None</code>: Do not load the <code>cpe</code> module but rely on explicit dependencies specified in     the list of dependencies instead.</p> </li> </ul> </li> <li> <p><code>cray_targets</code>: A list of Cray targetting modules to load.</p> </li> <li> <p><code>dependencies</code>:  This is a standard EasyConfig parameter. The versions of the     selected PrgEnv, compiler and <code>craype</code> module can be specified through dependencies     but those modules will still be loaded according to the scheme below. Any redifinition     of the <code>cpe</code> module is discarded.</p> </li> </ul>"},{"location":"CrayPEToolchain/#order-of-loads-generated-by-the-easyblock","title":"Order of loads generated by the EasyBlock","text":"<ol> <li> <p>The <code>cpe/&lt;CPE_version&gt;</code> module, if <code>CPE_load</code> is <code>first</code>.</p> <p>If LMOD would be modified to honour changes to <code>LMOD_MODULERCFILE</code> immediately as it does with changes to <code>MODULEPATH</code>, this would be the best moment to load the <code>cpe</code> module as it ensures that all other packages would be loaded with the correct version number immediately.</p> </li> <li> <p>The <code>PrgEnv-&lt;PrgEnv&gt;</code> module, if <code>PrgEnv_load</code> is True.</p> </li> <li> <p>The targeting modules specified by <code>cray_targets</code>. Hence they can overwrite the     targets set by the <code>PrgEnv-*</code> module which may be usefull on a heterogeneous     system should there only be a single configuration for the <code>PrgEnv-*</code> modules     for all hardware partitions in the system, or to build a <code>cpe*</code> module for cross-compiling.</p> <p>Note that changes to the targeting modules may trigger reloads of other modules loaded by the <code>PrgEnv-*</code> module.</p> </li> <li> <p>The <code>CPE_compiler</code> module (or autodected one), unless both     PrgEnv-* is loaded explicitly and the module is not in the list     of dependencies (in which case we rely on the <code>PrgEnv-*</code> module to do the proper     job).</p> </li> <li> <p>The craype module (compiler wrappers), unless both PrgEnv-* is     loaded explicitly and the module is not in the list of dependencies (in which case     we rely on the <code>PrgEnv-*</code> module to do the proper job).</p> </li> <li> <p>The specified dependencies, minus the <code>cpe/*</code>, <code>PrgEnv-*</code> and <code>craype/*</code>     modules.</p> </li> <li> <p>The <code>cpe/&lt;CPE_version&gt;</code> module, if <code>CPE_load</code> is <code>last</code>.</p> <p>In principle this should reload any module loaded before in a version that does not match the selected Cray PE version, and hence will also overwrite versions set in the dependencies. However, in the Cray PE 21.04 release (which was used for testing) the module did not always do the reloads in the proper order to always ensure the right version, and one might even end up with a version that is neither the one specified in the dependencies nor the one specified by the <code>cpe/*</code> module.</p> </li> </ol>"},{"location":"CrayPEToolchain/#some-examples","title":"Some examples","text":""},{"location":"CrayPEToolchain/#non-working-load-cpe-and-prgenv-gnu","title":"Non-working: Load cpe and PrgEnv-gnu","text":"<p>This is the default configuration for this EasyBlock. A minimal EasyConfig (omitting some mandatory parts such the <code>homepage</code> and <code>description</code> parameters) is</p> <p><pre><code>easyblock = 'CrayPEToolchain'\n\nname = 'cpeGNU'\nversion = \"21.04\"\n\ntoolchain = SYSTEM\n\nmoduleclass = 'toolchain'\n</code></pre> This generates a module file that activates the toolchain by only loading the <code>cpe/21.04</code> and <code>PrgEnv-gnu</code>-modules (in that order). Unfortunately, this scheme does not work with LMOD 8.3.x as is part of the Cray PE stack when the 21.04-21.06 releases were made, nor with version from the 8.4 and 8.5 branches, as LMOD_MODULERCFILE is only honoured at the next <code>module</code> call. If the effect of <code>LMOD_MODULERCFILE</code> would be immediate, this would probably be the most efficient way of activating a particular release of a particular PrgEnv. The module does not belong to any family. Instead it explicitly unloads other <code>cpe*</code> modules.</p>"},{"location":"CrayPEToolchain/#non-working-load-prgenv-gnu-and-then-cpe","title":"Non-working: Load PrgEnv-gnu and then cpe","text":"<p>Now we first load a <code>PrgEnv-*</code> module and only subsequently the <code>cpe/yy.mm</code> module that fixes versions for the modules. <pre><code>easyblock = 'CrayPEToolchain'\n\nname = 'cpeGNU'\nversion = \"21.04\"\n\ntoolchain = SYSTEM\n\nPrgEnv_family = 'cpeToolchain'\nCPE_load = 'after'\n\nmoduleclass = 'toolchain'\n</code></pre> This generates a module file that activates the toolchain by first loading the <code>PrgEnv-gnu</code> module and then correcting the versions by loading <code>cpe/21.04</code>. This doesn't work reliably either due to the current design of the module reloading process in the <code>cpe/21.04</code> module combined with the delayed impact of changes to <code>LMOD_MODULERCFILE</code>.</p> <p>The module will belong to the <code>cpeToolchain</code> family. That family will take care of unloading any other <code>cpe*</code> module that would be loaded (provided the <code>PrgEnv_family</code> parameter was set the same way in their EasyConfigs), while the <code>PrgEnv-gnu</code> module will take care of unloading other <code>PrgEnv-*</code> modules through the <code>PrgEnv</code> family.</p>"},{"location":"CrayPEToolchain/#a-setup-without-prgenv-or-cpe-module","title":"A setup without PrgEnv- or cpe module","text":"<p>On LUMI, due to the problems with LMOD and the cpe modules, we currently use a setup without <code>PrgEnv-*</code> or <code>cpe</code> module. One of the functions of the <code>cpe</code> module, setting the default versions of the Cray PE components, is already done by the <code>LUMI</code> module that loads the software stack. The other is replaced by hard-coding the necessary versions in the EasyConfig. One of the functions of the <code>PrgEnv-*</code> modules, setting and environment variable that tells the compiler wrappers which PE is selected, is taken over by the EasyBlock which sets the variable in the module file that it generates. The other, loading the correct targets and other PE modules, is taken over by the <code>craype_targets</code> parameter and the dependency list. This is the most reproducible setup as it only depends on versioned components (the <code>partition</code> module already ensures that a particular version of the Cray targeting modules is made available). <pre><code>easyblock = 'CrayPEToolchain'\n\nname = 'cpeGNU'\nversion = \"21.04\"\n\ntoolchain = SYSTEM\n\nPrgEnv_load = False\nPrgEnv_family = 'PrgEnv'\nCPE_load = None\n\ncray_targets = [\n    'craype-x86-rome',\n    'craype-accel-host',\n    'craype-network-ofi'\n]\n\ndependencies = [\n   ('gcc/9.3.0',              EXTERNAL_MODULE),\n   ('craype/2.7.6',           EXTERNAL_MODULE),\n   ('cray-mpich/8.1.4',       EXTERNAL_MODULE),\n   ('cray-libsci/21.04.1.1',  EXTERNAL_MODULE),\n   ('cray-dsmml/0.1.4',       EXTERNAL_MODULE),\n   ('perftools-base/21.02.0', EXTERNAL_MODULE),\n   ('xpmem',                  EXTERNAL_MODULE),\n]\n\nmoduleclass = 'toolchain'\n</code></pre> The <code>cpeGNU</code> module generated by this EasyConfig will be unloaded if the user would load a <code>PrgEnv-*</code> module as it is also a member of the <code>PrgEnv</code> family. As such it is a full replacement of the Cray <code>PrgEnv-gnu</code> module.</p>"},{"location":"CrayPEToolchain/#loading-the-cpe-and-prgenv-modules-first-then-reloading-packages-just-to-be-sure","title":"Loading the cpe and PrgEnv modules first, then reloading packages just to be sure","text":"<p>A compromise solution that will work around the problems with LMOD and the <code>cpe</code> modules yet retain much of the spirit of the Cray PE, and that also can correct the targeting modules should the <code>PrgEnv-*</code> module not take the ones that you want (or ensure that at least certain other modules are loaded, even if they would be removed from the list of modules loaded by <code>PrgEnv-gnu</code> in an update of the system), is the following setup: <pre><code>easyblock = 'CrayPEToolchain'\n\nname = 'cpeGNU'\nversion = '21.04'\n\ntoolchain = SYSTEM\n\nCPE_load = 'first'\nPrgEnv_load = True\nPrgEnv_family = 'cpeToolchain'\n\ncray_targets = [\n    'craype-x86-rome',\n    'craype-accel-host',\n    'craype-network-ofi'\n]\n\ndependencies = [\n   ('PrgEnv-gnu/8.0.0',       EXTERNAL_MODULE),\n   ('gcc/9.3.0',              EXTERNAL_MODULE),\n   ('craype/2.7.6',           EXTERNAL_MODULE),\n   ('cray-mpich/8.1.4',       EXTERNAL_MODULE),\n   ('cray-libsci/21.04.1.1',  EXTERNAL_MODULE),\n   ('cray-dsmml/0.1.4',       EXTERNAL_MODULE),\n   ('perftools-base/21.02.0', EXTERNAL_MODULE),\n   ('xpmem',                  EXTERNAL_MODULE),\n]\n\nmoduleclass = 'toolchain'\n</code></pre> This setup will first load the <code>cpe/21.04</code> and <code>PrgEnv-gnu/8.0.0</code> modules to stay in the Cray PE spirit. Next the indicated targeting modules will be loaded, one for the CPU, one for the accelerator architecture and one for the network. This may trigger reloads of some other modules and will overwrite targeting modules of the same type loaded by <code>PrgEnv-gnu</code>. Finally, the gcc compiler module, the craype module and all other modules from the dependency list are loaded with the versions specified.</p> <p>This setup is a compromise that on one hand stays close to the Cray PE spirit by using the <code>cpe</code> and <code>PrgEnv-gnu</code> modules, yet works around some problems, namely:   * Setting <code>LMOD_MODULERCFILE</code> does not work immediately.   * Any corrective action when loading <code>cpe</code> after <code>PrgEnv-gnu</code> does not work   * On a heterogeneous cluster, the targeting modules loaded by <code>PrgEnv-gnu</code> may     not be the ones you want when cross-compiling or when the system would use the same     file defining the modules for the whole system.   * The list of modules loaded by <code>PrgEnv-gnu</code> may change as it is determined by     a single file on the system that does not depend on the version of the Cray PE.     In this case, you can always be sure that at least the modules mentioned in the     dependency list and <code>cray_targets</code> parameter will be loaded.</p> <p>A variant of this would set <code>CPE_load = 'after'</code> which would load the <code>cpe/21.04</code> module immediately after loading <code>PrgEnv-gnu</code> rather than just before, but with the current flaws of the <code>cpe/21.04</code> module this still does not solve all problems:</p> <pre><code>easyblock = 'CrayPEToolchain'\n\nname = 'cpeGNU'\nversion = '21.04'\n\ntoolchain = SYSTEM\n\nCPE_load = 'after'\nPrgEnv_load = True\nPrgEnv_family = 'cpeToolchain'\n\ncray_targets = [\n    'craype-x86-rome',\n    'craype-accel-host',\n    'craype-network-ofi'\n]\n\ndependencies = [\n   ('PrgEnv-gnu/8.0.0',       EXTERNAL_MODULE),\n   ('gcc/9.3.0',              EXTERNAL_MODULE),\n   ('craype/2.7.6',           EXTERNAL_MODULE),\n   ('cray-mpich/8.1.4',       EXTERNAL_MODULE),\n   ('cray-libsci/21.04.1.1',  EXTERNAL_MODULE),\n   ('cray-dsmml/0.1.4',       EXTERNAL_MODULE),\n   ('perftools-base/21.02.0', EXTERNAL_MODULE),\n   ('xpmem',                  EXTERNAL_MODULE),\n]\n\nmoduleclass = 'toolchain'\n</code></pre>"},{"location":"CrayPEToolchain/#mimic-prgenv-load-hardcoded-versions-but-load-cpeyymm-first","title":"Mimic PrgEnv, load hardcoded versions but load cpe/yy.mm first","text":"<p>This is yet another compromise scenario:   * Loading <code>cpe/yy.mm</code> first ensures that further modules a user might load after     loading the <code>cpe*</code> module will load in the proper versions if a user does a versionless     load.   * Mimicing <code>PrgEnv-*</code> and loading modules explicitly ensures reproducibility over     time as the list of modules loaded does not depend on a single file elsewhere in     the system configuration which is not specific to a particular release of the PE.   * Hard-coding the versions ensures that we avoid the problems caused by the implementation     of the <code>cpe/yy.mm</code> modules (certainly in releases up to and including 21.06)</p> <pre><code>easyblock = 'CrayPEToolchain'\n\nname = 'cpeGNU'\nversion = '21.04'\n\ntoolchain = SYSTEM\n\nPrgEnv_load = False\nPrgEnv_family = 'PrgEnv'\nCPE_load = 'first'\n\ncray_targets = [\n    'craype-x86-rome',\n    'craype-accel-host',\n    'craype-network-ofi'\n]\n\ndependencies = [\n   ('PrgEnv-gnu/8.0.0',       EXTERNAL_MODULE),\n   ('gcc/9.3.0',              EXTERNAL_MODULE),\n   ('craype/2.7.6',           EXTERNAL_MODULE),\n   ('cray-mpich/8.1.4',       EXTERNAL_MODULE),\n   ('cray-libsci/21.04.1.1',  EXTERNAL_MODULE),\n   ('cray-dsmml/0.1.4',       EXTERNAL_MODULE),\n   ('perftools-base/21.02.0', EXTERNAL_MODULE),\n   ('xpmem',                  EXTERNAL_MODULE),\n]\n\nmoduleclass = 'toolchain'\n</code></pre>"},{"location":"CrayPEToolchain/#mimic-prgenv-and-load-cpeyymm-at-the-end","title":"Mimic PrgEnv and load cpe/yy.mm at the end","text":"<p>This would be a valid scenario once the <code>cpe/yy.mm</code> modules have been corrected and work as they should. In this scenario,</p> <ul> <li> <p>We mimic <code>PrgEnv-*</code> by setting the necessary environment variables and then loading     a list of versionless modules. This avoids a problem with the actual PrgEnv modules     as the list of modules they load depends on a single system file which is the same     for all releases of the PE and hence may change over time.</p> </li> <li> <p>At the end the relevant <code>cpe/yy.mm</code> module is loaded to fix the versions of all     already loaded modules.</p> </li> </ul> <p>The corresponding EasyConfig file (minus help etc.) is: <pre><code>easyblock = 'CrayPEToolchain'\n\nname = 'cpeGNU'\nversion = '21.04'\n\ntoolchain = SYSTEM\n\nPrgEnv_load = False\nPrgEnv_family = 'PrgEnv'\nCPE_load = 'last'\n\ncray_targets = [\n    'craype-x86-rome',\n    'craype-accel-host',\n    'craype-network-ofi'\n]\n\ndependencies = [\n   ('gcc',            EXTERNAL_MODULE),\n   ('craype',         EXTERNAL_MODULE),\n   ('cray-mpich',     EXTERNAL_MODULE),\n   ('cray-libsci',    EXTERNAL_MODULE),\n   ('cray-dsmml',     EXTERNAL_MODULE),\n   ('perftools-base', EXTERNAL_MODULE),\n   ('xpmem',          EXTERNAL_MODULE),\n]\n\nmoduleclass = 'toolchain'\n</code></pre></p>"},{"location":"CrayPE_integration/","title":"Cray PE integration","text":""},{"location":"CrayPE_integration/#cray-pe-components-and-the-configuration-of-the-module-system-and-easybuild","title":"Cray PE components and the configuration of the module system and EasyBuild","text":"<p>At several places in the configuration of the module system and of EasyBuild, information is needed about version numbers of CPE components per CPE release. For now, this information (and in particular version numbers) of CPE components is provided through a a .csv-file which is edited by hand and stored in the CrayPE subdirectory. It would be possible to extract some of the information in that file from parsing \\ <code>/opt/cray/pe/cpe/yy.mm/modulerc.lua</code>. In the future, HPE-Cray may deliver that information in a more easily machiine-readable format.</p> <p>There are several points where we need information about versions of specific packages in releases of the Cray PE:</p> <ul> <li> <p>We need to define external modules to EasyBuild.</p> <p>That file is currently generate by <code>make_EB_external_modules.py</code> which is a wrapper that calls <code>lumitools/gen_EB_external_modules_from_CPEdef.py</code> to generate the EasyBuild external modules file for a particular version of the CPE. The external module definition file is stored in the easybuild configuration directory and is called <code>external_modules_metadata-CPE-&lt;CPE version&gt;.cfg</code>. Hence in the current design it is named after the version of the CPE and not the name of the software stack, so the developer and release versions of a software stack would share the same file.</p> </li> <li> <p>For now, as the cpe module cannot work correctly due to restrictions in LMOD,     we need to specify the exact versions of packages in the various cpe* toolchain     easyconfigs.</p> <p>In a later phase this may not be needed anymore though we might still want to avoid relying on the PrgEnv-* modules as the only source for the toolchain components as the components included via that file might change over time and are determined by a single file on the system which has to be the same for all releases.</p> </li> <li> <p>For now we need to overwrite the Cray PE cpe module for the release of the CPE.</p> <p>This is needed because the <code>cpe</code> module sets the LMOD_MODULERCFILE environment variable, a problem that should be solved from release 21.08 onwards. Moreover, our version implements a better strategy to reload modules so that (re)loading a <code>cpe</code> module at the end of the configuration of the PE will always set the right versions. But HPE Cray will use that method also starting from 21.08 onwards so that our own <code>cpe</code> modules will no longer be needed.</p> <p>We use a generic implementation of the module file that simply reads the .csv file to find out component versions.</p> </li> <li> <p>A module avail hook to hide those modules that are irrelevant to a particular LUMI/yy.mm     toolchain could also use that information.</p> <p>For efficiency reasons this hook actually uses a LUA file which is generated from the .csv file with the components.</p> <p>Note that such a module only unclutters the display for users. A hidden module can still be loaded, and if that module is marked as default it would actually be loaded instead of another module with the same name and version.</p> </li> <li> <p>The information is also used to generate a modulerc file for each particular version     of the LUMI/yy.mm software stack to mark the specific versions of the Cray PE modules     for that release as the default (effectively already doing part of the work of     the <code>cpe</code> modules when loading the software stack module).</p> </li> <li> <p>Within the software stack module we ensure that we load the matching version of     the Cray targeting modules.</p> </li> <li> <p>We may need it to define Cray PE components to Spack</p> <p>Not developed yet.</p> </li> </ul>"},{"location":"SitePackage/","title":"LUMI prototype SitePackage.lua","text":"<p>SitePackage.lua is used to implement various customistations to Lmod.</p> <p>The file defines both LMOD hooks and a number of new functions for the sandbox to ease the implementation of modulefiles for LUMI.</p> <p>Make sure that the environment variable <code>LMOD_PACKAGE_PATH</code> points to the directory that contains this file to activate this file.</p>"},{"location":"SitePackage/#settings","title":"Settings","text":""},{"location":"SitePackage/#target-modules-per-partition","title":"Target modules per partition","text":"<p>The target modules per partition are set via the init_module_list table. This also includes other modules that are set at initialisation.</p>"},{"location":"SitePackage/#default-programming-environment","title":"Default programming environment","text":"<p>The default programming environment is set via the init_PrgEnv variable.</p>"},{"location":"SitePackage/#list-of-lts-software-stacks","title":"List of LTS software stacks","text":"<p>In the initial implementation every software stack that was not a development stack was marked as \"LTS\" but it turns out it will be very difficult initially to live up to the promise of 2 years of support. We simply need to change the programming environment too often and HPE Cray also does not keep supporting them.</p> <p>The LUMI-stacks with long-term support are set in LTS_LUMI_stacks</p>"},{"location":"SitePackage/#hooks","title":"Hooks","text":""},{"location":"SitePackage/#sitename-hook","title":"SiteName hook","text":"<p>This hook is used to define the prefix for some of the environment variables that Lmod will generate internally. Rather then just setting it to <code>LUMI</code> we decided to set it to <code>LUMI_LMOD</code> to lower the chance of conflicts with environment variables that may be defined elsewhere.</p>"},{"location":"SitePackage/#avail-hook","title":"avail hook","text":"<p>This hook is used to replace directories with labels in the output of <code>module avail</code>.</p> <p>To work for the prototypes, one needs to set: <pre><code>export LMOD_AVAIL_STYLE=&lt;label&gt;:PEhierarchy:system\n</code></pre> which will make the labeled view the default but will still allow to see the directory view using <pre><code>module -s system avail\n</code></pre></p> <p>The <code>ModuleLabel</code> modules can also be used to switch between the three settings:</p> <ul> <li> <p><code>label</code>: User-friendly labels for the directories, some directories collapsed into     a single category, and all Cray PE modules collapsed into a single category.</p> </li> <li> <p><code>PEhierarchyy</code>: User-friendly labels for the directories, some directories collapsed into     a single category, but with the original Cray PE module hierarchy.</p> </li> <li> <p><code>system</code>: The view showing the full directory path for each module directory.</p> </li> </ul>"},{"location":"SitePackage/#msghook","title":"msgHook","text":"<p>This hook is used to adapt the following messages:   * output of <code>module avail</code>:  Add more information about how to search for software     and to contact LUMI User Support.</p>"},{"location":"SitePackage/#visibility-hook","title":"Visibility hook","text":"<p>The visibility hook is currently used to hide the Cray modules of other CPE versions in a particular version of the CPE software stack. It is also used to hide the  <code>EasyBuild</code> modules as users sometimes load these without loading the  <code>EasyBuild-user</code> module.</p> <p>As the hook function is called many times during a single call to <code>module avail</code> some effort was done to make it efficient at the cost of readability.</p> <ul> <li> <p>The data about the Cray PE modules that should not be hidden is contained in a LUA script     in <code>mgmt/LMOD/VisibilityHookData</code>. This script is read via <code>require</code> so that     it is cached and really processed only once. Furthermore, to make locating the     script easy, the LUMI stack module stores the path and name in two environment     variables that are ready-to-use without further substitutions.</p> <p>The file is auto-generated during the initialisation of a software stack.</p> <p>The name and location of the file is set through two environment variables set in the LUMI modules.</p> </li> <li> <p>The code to hide EasyBuild is currently rather rudimentary. It doesn't try to do      so only for EasyBuild modules in the central software stack, but would hide an     EasyBuild module that has been installed by a user also. This is partly the     result of design flaws making it harder to distinguish user and central      modules.</p> </li> <li> <p>Note that the feature is turned of for power users.</p> </li> </ul>"},{"location":"SitePackage/#custom-lmod-functions","title":"Custom LMOD functions","text":""},{"location":"SitePackage/#detect_lumi_partition","title":"detect_LUMI_partition","text":"<p><code>detect_LUMI_partition</code>is a function that can be used to request the current LUMI partition. This is used by the generic <code>modules/LUMIstack</code> modules files but put in <code>SitePackage.lua</code> to have a single point where this is implemented.</p> <p>The alternative would be to use a trick that is also used in some CPE module files to read in and execute code from an external file.</p> <p>LUMI_OVERWRITE_PARTITION is defined and if so, the value of that variable is used. It is assumed to be C, G, D or L depending on the node type (CPU compute, GPU compute, data and visualisation or login), or can be common to install software in the hidden common partition.</p> <p>Currently the following rules are implemented for LUMI:</p> <ul> <li> <p>Node name starts with <code>uan</code>:  It must be a login node, so in partition/L</p> </li> <li> <p>Nude number 16-23: LUMI-D with GPU</p> </li> <li> <p>Node number 101-108: LUMI-D without GPY, assing to partition/L</p> </li> <li> <p>Node number 1000-2535: Regular LUMI-C compute nodes, assign partition/C</p> </li> <li> <p>Other nodes are for now assigned to partition/L.</p> </li> </ul> <p>The idea is to ensure that a <code>module update</code> would reload the loaded software stack for the partition on which the <code>module update</code> command is run.</p>"},{"location":"SitePackage/#get_cpe_component","title":"get_CPE_component","text":"<p><code>get_CPE_component</code> is a function that can be used in modulefiles to request the version of a CPE component. The data is read from the CPE definition files in the <code>CrayPE</code> subdirectory of the repository.</p> <p>The function is currently used in the <code>modules/LUMIpartition</code> generic implementation of the partition modules for the LUMI stacks to determine the version of the Cray targeting modules to add that directory to the MODULEPATH.</p>"},{"location":"SitePackage/#get_cpe_versions","title":"get_CPE_versions","text":"<p><code>get_CPE_versions</code> is a function that can be used in module files to request a table with the version for each package in a CPE release. The data is read from the CPE definition files in the <code>CrayPE</code> subdirectory of the repository.</p> <p>The function is used in the prototype in the <code>cpe</code> modules for the Grenoble system as a proof-of-concept for a generic <code>cpe</code> module to reduce the number of places where the version info of the Cray packages in a CPE release is kept.</p>"},{"location":"SitePackage/#get_easybuild_version","title":"get_EasyBuild_version","text":"<p><code>get_EasyBuild_version</code> is a function that simply returns the version of  EasyBuild for a given software stack (and actually a given CPE version in the current implementation, so a <code>.dev</code> stack is assumed to have the same version of EasyBuild as the regular stack). Currently we abuse the CPE definition files in <code>CrayPE</code> to set the version of EasyBuild, but by making this a separate function this can be changed in the future.</p>"},{"location":"SitePackage/#get_versionedfile","title":"get_versionedfile","text":"<p><code>get_versionedfile</code> is a function that can be used to find the suitable versioned file for a given version of the LUMI software stack, i.e., the file which according to the version number is the most recent one not newer than the software stack.</p>"},{"location":"SitePackage/#get_hostname","title":"get_hostname","text":"<p><code>get_hostname</code> gets the hostname from the output of the <code>hostname</code> command. It is meant to be used by detect_LUMI_partition but is also exported so that other module files can use it if needed.</p>"},{"location":"SitePackage/#get_user_prefix_easybuild","title":"get_user_prefix_EasyBuild","text":"<p><code>get_user_prefix_EasyBuild</code> computes the root of the user EasyBuild installation from the environment variable <code>EBU_USER_PREFIX</code> and the default name in the home directory.</p> <p>It is used in the <code>EasyBuild-config</code> module, the <code>LUMIpartition</code> module (to include the user module directory in the <code>MODULEPATH`) and in the</code>avail_hook`` LMOD hook.</p>"},{"location":"SitePackage/#get_init_module_list","title":"get_init_module_list","text":"<p><code>get_init_module_list</code> is a function that returns the list of modules to load at initialisation. This includes target modules, other modules, and optionally the default programming environment.</p> <p>The function takes two arguments:</p> <ol> <li> <p>The partition (L, C, G, D)</p> </li> <li> <p>A boolean: When true the default programming environment is added to the list of     modules that is generated as a result of the function.</p> </li> </ol>"},{"location":"SitePackage/#get_motd","title":"get_motd","text":"<p><code>get_motd</code> returns the message-fo-the-day as stored in <code>etc/motd</code> in the repository root. The function takes no input arguments.</p>"},{"location":"SitePackage/#get_fortune","title":"get_fortune","text":"<p><code>get_fortune</code> works as the old UNIX <code>fortune</code> command. It returns a random tip read from <code>etc/lumi_fortune.txt</code> in the repository root.</p>"},{"location":"SitePackage/#is_interactive","title":"is_interactive","text":"<p><code>is_interactive</code> returns true if it is called in an interactive shell, otherwise false. The function takes no input arguments.</p> <p>It is used to ensure that the message-of-the-day is not printed in cases where Linux would not print it.</p>"},{"location":"SitePackage/#is_lts_lumi_stack","title":"is_LTS_LUMI_stack","text":"<p><code>is_LTS_LUMI_stack</code> takes one input argument: the version of the LUMI stack. It returns true if that version is a LTS stack and returns false otherwise.</p>"},{"location":"SitePackage/#get_container_repository_root","title":"get_container_repository_root","text":"<p><code>get_container_repository_root</code> takes no input arguments. It returns the location of the container recipe which is taken from the <code>LUMI_CONTAINER_REPOSITORY_ROOT</code> when defined or the default <code>/appl/local/containers</code> otherwise.</p>"},{"location":"SitePackage/#get_eb_container_repository","title":"get_EB_container_repository","text":"<p><code>get_EB_container_repository</code> takes no input arguments. It assembles the name of the directory of the container repository that EasyBuild should use from the result of <code>get_container_repository_root</code>. Its main function is to have the name of the subdirectory in only one place of the code to have more flexibility to change it when needed.</p>"},{"location":"SitePackage/#get_sif_file","title":"get_SIF_file","text":"<p><code>get_SIF_file()</code> takes three arguments:</p> <ol> <li> <p>Name of the SIF file</p> </li> <li> <p>Name of the package (EasyBuild easyconfig)</p> </li> <li> <p>Installation directory (can come from %(installdir)s in an EasyConfig)</p> </li> </ol> <p>This routine checks first if the .sif file is present in the installation directory. If not, it computes the location from the name of the .sif file, the EasyBuild package name (in case we want to implement a structure like p/PyTorch), and whatever it gets from the get_container_repository function.</p>"},{"location":"SitePackage/#convert_to_ebvar","title":"convert_to_EBvar","text":"<p>The <code>convert_to_EBvar</code> outine is used to create an EasyBuild-style variable name from the package name and an optional prefix and postfix. It takes three arguments:</p> <ol> <li> <p><code>package</code>: Name of the package (EasyBuild-style)</p> </li> <li> <p><code>prefix</code>: Optional prefix for the generated variable name (can be <code>nil</code>)</p> </li> <li> <p><code>postfix</code>: Optional postfix for the generated variable name (cam be <code>nil</code>)</p> </li> </ol> <p>Output: The name of the variable generated from the package, by turning all characters uppercase and replacing a hyphen with MIN, with they prefix in front and the postfix at the end of the string.</p> <p>This is not yet a complete equivalent of the EasyBuild variable-from-packages generator, but it is a start for now.</p>"},{"location":"SitePackage/#create_container_vars","title":"create_container_vars","text":"<p>The <code>create_container_vars</code> routine sets a number of environment variables in the modules that we use to make some containers available.</p> <p>It takes three input arguments, with the last two optional:</p> <ol> <li> <p><code>sif_file</code>: Name of the SIF file</p> </li> <li> <p><code>package_name</code>: Name of the package (EasyBuild easyconfig)</p> </li> <li> <p><code>installdir</code>: Installation directory (can come from %(installdir)s in an EasyConfig)</p> </li> </ol> <p>Return value: None. It simply calls a Lmod functions to set some environment variables.</p>"},{"location":"configurations/","title":"Configurations used on LUMI","text":"Cray PE EasyBuild Toolchains 21.08 4.4.2 cpeCray, cpeGNU 21.12 4.5.3 cpeCray, cpeGNU, cpeAOCC"},{"location":"directory_structure/","title":"The directory structure","text":""},{"location":"directory_structure/#system-directory","title":"System directory","text":"<p>Most of this hierarchy is created with the <code>prepare_LUMI.sh</code> script.</p> <p>The Cray PE modules are left in their own hierarchy.</p> <p>In the LUMI software stack installation directory, one can find the following subdirectories:</p> <ul> <li> <p><code>modules</code>: Key here was to follow Lmod guidelines on hierarchical structures for     the software stack itself, though once at the level of the software we currently     do not use a hierarchy.</p> <ul> <li> <p><code>SoftwareStack</code>: A module file that enables the default Cray environment, and one     for each of our LUMI software stacks, of the form LUMI/version.lua.</p> </li> <li> <p><code>SystemPartition/LUMI/yy.mm</code>: The next level in the hierarchy. It contains modules     by LUMI SoftwareStack to enable the different partitions. Structure:</p> <ul> <li> <p>The modules are called <code>partition/C.lua</code> etc.</p> <p>We could not use LUMIpartition as this lead to problems with the Lmod hierachyA function which produced wrong results for LUMIpartition/L.lua but not for LUMIpartition/C.lua even though both modules had identical code.</p> </li> <li> <p>Besides the four regular partitions, there is also a meta-partition common that is     used to house software that is common to all regular partitions. The corresponding     module is hidden from regular users.</p> </li> </ul> </li> <li> <p><code>Infrastructure/LUMI/yy.mm/partition/part</code>: Infrastructure modules. This structure     is needed for those modules of which we need versions of each of the regular     partition and for the common partition. This does include the modules that     are used for EasyBuild settings.</p> </li> <li> <p><code>easybuild/LUMI/yy.mm/partition/part</code>: Directory for the EasyBuild-generated modules     for the LUMI/yy.mm software stack for the LUMI-part partition (part actually being     a single letter, except for the software that is common to all partitions, where     part is common)</p> </li> <li> <p><code>easybuild/CrayEnv</code>: Directory for the EasyBuild-generated modules for the     CrayEnv software stack.</p> </li> <li> <p><code>easybuild/system</code>: Directory for the EasyBuild-generated modules outside     any software stack</p> </li> <li> <p><code>spack/LUMI/yy.mm/partition/part/&lt;archstring&gt;</code>: Similar as the above, but for Spack-installed software.</p> </li> <li> <p><code>manual/LUMI/yy.mm/partition/part</code>: Similar as the above, but for manually installed     software.</p> </li> <li> <p><code>CrayOverwrite</code>: A directory that is currently used to implement modules     that are missing on our test system in Grenoble and to work around some of     the problems in the Cray <code>cpe/yy.mm</code> modules.</p> <p>Note that these modules seem to give other problems so though we still put them on the system they are currently disabled.</p> </li> <li> <p><code>StyleModifiers</code>: Links to the corresponding module in the repository. It     contains the modules that can be used to change the presentation of the modules     in <code>module avail</code>.</p> </li> </ul> </li> <li> <p>SW : This is for the actual binaries, lib directories etc. Names are deliberately kept short to     avoid problems with too long shebang lines. As shebang lines do not undergoe variable expansion,     we cannot use the EBROOT variables and so on in those lines to save space.</p> <ul> <li> <p><code>LUMI-yy.mm</code></p> <ul> <li> <p><code>C</code></p> <ul> <li> <p><code>EB</code></p> </li> <li> <p><code>SP</code></p> </li> <li> <p><code>MNL</code></p> </li> </ul> </li> <li> <p><code>G</code></p> </li> <li> <p><code>D</code></p> </li> <li> <p><code>L</code></p> </li> <li> <p><code>common</code></p> </li> </ul> </li> <li> <p><code>CrayEnv</code></p> </li> <li> <p><code>system</code></p> </li> </ul> </li> <li> <p><code>mgmt</code>: Files that are not stored in our GitHub, but are generated on the fly and are only     useful to those users who want to build upon our software stack or for those who install     software in our stacks.</p> <ul> <li> <p><code>ebrepo_files</code></p> <ul> <li> <p><code>LUMI-yy.mm</code></p> <ul> <li> <p><code>LUMI-C</code></p> </li> <li> <p><code>LUMI-G</code></p> </li> <li> <p><code>LUMI-D</code></p> </li> <li> <p><code>LUMI-L</code></p> </li> <li> <p><code>LUMI-common</code></p> </li> </ul> </li> <li> <p><code>CrayEnv</code></p> </li> <li> <p><code>system</code></p> </li> </ul> </li> <li> <p><code>LMOD</code> : Additional files for LMOD</p> <ul> <li><code>VisibilityHookData</code> : Auto-generated files used by the LMOD <code>SitePackage.lua</code>     file to hide Cray modules that are irrelevant for a particular software     stack.</li> </ul> </li> </ul> </li> <li> <p><code>sources</code>: Directory structure to store sources of installed programs so that they     can be reinstalled even if sources would no longer be downloadable</p> <ul> <li> <p><code>easybuild</code>: Sources downloaded by EasyBuild. The internal structure of     this directory is determined by EasyBuild.</p> </li> <li> <p>Further subdirectories are not fixed yet, but the suggestion is to also provide     a <code>manual</code> subdirectory for the sources of those packages that are installed     manually and a <code>spack</code> subdirectory for spack-installed software when we     proceed with the Spack integration.</p> </li> </ul> </li> <li> <p>SystemRepo: GitHub repository with all managed files. The name is not fixed, any     name can be used and will be picked up if the scripts from the <code>scripts</code> subdirectory     inside the repository are used to initialise a new software stack or to determine     the values for some of the environment variables for LMOD.</p> <p>For the structure inside the repository, see the \"overview of files in the repository and where they are being used.</p> </li> <li> <p><code>LUMI-EasyBuild-contrib</code> (optional and not created by the script): A clone of the     LUMI-EasyBuild-contrib repository     only used for search in EasyBuild.</p> </li> </ul>"},{"location":"directory_structure/#user-easybuild-setup","title":"User EasyBuild setup","text":"<p>This is a very simplified version of the system directory structure with levels in the directory structure omitted when they don't make sense as this structure is for EasyBuild installations only.</p> <ul> <li> <p><code>modules/LUMI/yy.mm/partition/part</code>: Directory for the EasyBuild-generated modules     for the LUMI/yy.mm software stack for the LUMI-part partition (part actually being     a single letter, except for the software that is common to all partitions, where     part is common)</p> </li> <li> <p><code>SW/LUMI-yy.mm/part</code> : This is for the actual binaries, lib directories etc.     Names are deliberately kept short to avoid problems with too long shebang lines.     As shebang lines do not undergoe variable expansion, we cannot use the EBROOT     variables and so on in those lines to save space.</p> <p>As for the modules directory, <code>part</code> is <code>C</code>, <code>G</code>, <code>D</code>, <code>L</code>or <code>common</code>.</p> </li> <li> <p><code>ebrepo_files/LUMI-yy.mm/LUMI-part</code> for the EasyBuild repository of installed     EasyConfigs in the user directory.</p> <p>As for the modules directory, <code>part</code> is <code>C</code>, <code>G</code>, <code>D</code>, <code>L</code>or <code>common</code>.</p> </li> <li> <p><code>sources</code>: Subdirectory where EasyBuild stores sources of isntalled packages.     The internal structure is fully determined by EasyBuild.</p> </li> <li> <p><code>UserRepo</code>: The user EasyBuild repo. Contrary to the repository in the system     directories, the name <code>UserRepo</code> is mandatory here.</p> <p>Subdirectories are</p> <ul> <li> <p><code>easybuild/config</code> to add to the system config files (the <code>easybuild-user.cfg</code>     and <code>easybuild-user-LUMI-yy.mm.cfg</code> files, see the     Setup of a LUMI software stack and EasyBuild page)</p> </li> <li> <p><code>easybuild/easyblkocks</code> for additional custom EasyBlocks. We currently assume     an organisation in two levels (first letter and then the python file).</p> </li> <li> <p><code>easybuild/easyconfigs</code> for the EasyConfigs.</p> </li> </ul> </li> </ul>"},{"location":"easybuild_setup/","title":"EasyBuild setup","text":""},{"location":"easybuild_setup/#configuration-decisions","title":"Configuration decisions","text":""},{"location":"easybuild_setup/#easybuild-module-naming-scheme","title":"EasyBuild Module Naming Scheme","text":"<ul> <li> <p>Options</p> <ul> <li> <p>A flat naming scheme, even without the module classes as they are of little     use. May packages belong to more than one class, it is impossible to come up     with a consistent categorization. Fully omitting the categorization requires     a slightly customized naming scheme that can be copied from UAntwerpen. When     combining with --suffix-modules-path='' one can also drop the 'all' subdirectory     level which is completely unnecessary in that case.</p> </li> <li> <p>A hierarchical naming scheme as used at CSCS and CSC. Note that CSCS has an     open bug report at the time of writing (May 12) on the standard implementation     in EasyBuild (Issue #3626     and the related issue 3575).     The solution might be to develop our own naming scheme module.</p> </li> </ul> </li> <li> <p>Choice implemented for the current software stacks:</p> <ul> <li> <p>A flat naming scheme that is a slight customization of the default EasyBuildMNS     without the categories, combined with an empty <code>suffix-modules-path</code> to avoid     adding the unnecessary <code>all</code> subdirectory level in the module tree.</p> </li> <li> <p>As we need to point to our own module naming scheme implementation which is     hard to do in a configuration file (as the path needs to be hardcoded), the     settings for the module scheme are done via <code>EASYBUILD_*</code> environment variables,     specifically:</p> <ul> <li><code>EASYBUILD_INCLUDE_MODULE_NAMING_SCHEMES</code> to add out own module naming schemes</li> <li><code>EASYBUILD_MODULE_NAMING_SCHEME=LUMI_FlatMNS</code></li> <li><code>EASYBUILD_SUFFIX_MODULES_PATH=''</code></li> </ul> </li> </ul> </li> </ul>"},{"location":"easybuild_setup/#other-configuration-decisions","title":"Other configuration decisions","text":"<ul> <li> <p>TODO: rpath or not?</p> </li> <li> <p>TODO: Hiding many basic libraries</p> </li> </ul>"},{"location":"easybuild_setup/#external-modules-for-integration-with-the-cray-pe","title":"External modules for integration with the Cray PE","text":"<p>See the Cray PE integration page.</p>"},{"location":"easybuild_setup/#running-easybuild","title":"Running EasyBuild","text":"<p>EasyBuild for LUMI is configured through a set of EasyBuild configuration files and environment variables. The basic idea is to never load the EasyBuild module directly without using one of the EasyBuild configuration modules. There are three such modules</p> <ul> <li> <p><code>EasyBuild-production</code> to do the installations in the production directories.</p> </li> <li> <p><code>EasyBuild-infrastructure</code> is similar to <code>EasyBuild-production</code> but places the module     in the Infrastructure tree rather than the easybuild tree.</p> </li> <li> <p><code>EasyBuild-user</code> is meant to do software installations in the home directory of a     user or in their project directory. This module will configure EasyBuild such that     it builds on top of the software already installed on the system, with a compatible     directory structure.</p> <p>This module is not only useful to regular users, but also to LUST to develop EasyConfig files for installation on the system. We aim to develop the module in such a way that being able to install the module in a home or project subdirectory would also practically guarantee that the installation will also work in the system directories.</p> </li> </ul> <p>These three modules are implemented as one generic module, <code>EasyBuild-config</code>, that is symlinked to those three modules. The module will derive what it should do from its name and also gets all information about the software stack and partition from its place in the module hierarchy to ensure maximum robustness.</p> <p>NOTE: We deliberately chose to use EasyBuild-production, EasyBuild-user etc and not EasyBuild-config/production, EasyBuild-config/user, etc. because of the way EasyBuild acts when loading a different partition module. For additional safety we want to avoid that LMOD would reload a different version of the EasyBuild-config module and would rather get an error message that the EasyBuild-* partition is not available in the new partition.</p>"},{"location":"easybuild_setup/#common-settings-that-are-made-through-environment-variables-in-all-modes","title":"Common settings that are made through environment variables in all modes","text":"<ul> <li> <p>The buildpath and path for temporary files. The current implementation creates     subdirectories in the directory pointed to by <code>XDG_RUNTIME_DIR</code> as this is a     RAM-based file system and gets cleaned when the user logs out. This value is based     on the CSCS setup.</p> </li> <li> <p>Name and location of the EasyBuild external modules definition file.</p> </li> <li> <p>Settings for the module naming scheme: As we need to point to a custom implementation     of the module naming schemes, this is done through an environment variable. For     consistency we also set the module naming scheme itself via a variable and set     EASYBUILD_SUFFIX_MODULES_PATH as that together with the module naming scheme determines     the location of the modules with respect to the module install path.</p> </li> <li> <p><code>EASYBUILD_OPTARCH</code> has been extended compared to the CSCS setup:</p> <ul> <li> <p>We support multiple target modules so that it is possible to select both the     CPU and accelerator via <code>EASYBUILD_OPTARCH</code>. See the     EasyBuild CPE toolchains common options</p> </li> <li> <p>It is now also possible to specify arguments for multiple compilers. Use <code>CPE:</code>     to mark the options for the CPE toolchains. See also     EasyBuild CPE toolchains common options</p> </li> </ul> </li> <li> <p>As the CPE toolchains are not included with the standard EasyBuild distribution     and as we have also extended them (if those from CSCS would ever be included),     we set <code>EASYBUILD_INCLUDE_TOOLCHAINS</code> to tell EasyBuild where to find the toolchains.</p> </li> </ul>"},{"location":"easybuild_setup/#the-easybuild-production-and-easybuild-infrastructure-mode","title":"The EasyBuild-production and EasyBuild-infrastructure mode","text":"<p>Most of the settings for EasyBuild on LUMI are controlled through environment variables as this gives us more flexibility and a setup that we can easily redo in a different directory (e.g., to test on the test system). Some settings that are independent of the directory structure and independent of the software stack are still done in regular configuration files.</p> <p>There are two regular configuration files:</p> <ol> <li> <p><code>easybuild-production.cfg</code> is always read. In the current implementation it is     assumed to be present.</p> </li> <li> <p><code>easybuild-production-LUMI-yy.mm.cfg</code> is read after <code>production.cfg</code>, hence can be used     to overwrite settings made in <code>production.cfg</code> for a specific toolchain. This     allows us to evolve the configuration files while keeping the possibility to install     in older versions of the LUMI software stack.</p> </li> </ol> <p>Settings made in the configuration files:</p> <ul> <li> <p>Modules tool and modules syntax.</p> </li> <li> <p>Modules that may be loaded when EasyBuild runs</p> </li> <li> <p>Modules that should be hidden. Starting point is currently the list of CSCS.     TODO: Not yet enabled as this makes development more difficult.</p> <p>In fact, another (untested) option is to hide the modules via a modulerc file in the module system rather than via EasyBuild which would have the advantage that they maintain regular version numbers rather than version numbers that start with a dot (as it seems that that version number with the dot should then also be used consistently).</p> </li> <li> <p>Ignore EBROOT variables without matching module as we use this to implement Bundles     that are detected by certain EasyBlocks as if each package included in the Bundle     was installed as a separate package.</p> </li> </ul> <p>The following settings are made through environment variables:</p> <ul> <li> <p>Software and module install paths, according to the directory scheme given in the     module system section.</p> </li> <li> <p>The directory where sources will be stored, as indicated in the directory structure     overview.</p> </li> <li> <p>The repo directories where EasyBuild stores EasyConfig files for the modules that     are build, as indicated in the directory structure overview.</p> </li> <li> <p>EasyBuild robot paths: we use EASYBUILD_ROBOT_PATHS and not EASYBUILD_ROBOT so     searching the robot path is not enabled by default but can be controlled through     the <code>-r</code> flag of the <code>eb</code> command. The search order is:</p> <ol> <li> <p>The repository for the currently active partition build by EasyBuild for     installed packages (<code>ebrepo_files</code>)</p> </li> <li> <p>The repository for the common partition build by EasyBuild for     installed packages (<code>ebrepo_files</code>)</p> </li> <li> <p>The LUMI-specific EasyConfig directory.</p> </li> </ol> <p>We deliberately put the ebrepo_files repositories first as this ensure that EasyBuild will always find the EasyConfig file for the installed module first as changes may have been made to the EasyConfig in the LUMI EasyConfig repository that are not yet reflected in the installed software.</p> <p>The default EasyConfig files that come with EasyBuild are not put in the robot search path for two reasons:</p> <ol> <li> <p>They are not made for the Cray toolchains anyway (though one could of course     use <code>--try-toolchain</code> etc.)</p> </li> <li> <p>We want to ensure that our EasyConfig repository is complete so that we can     impose our own standards on, e.g., adding information to the help block or     whatis lines in modules, and do not accidentally install dependencies without     realising this.</p> </li> </ol> </li> <li> <p>Names and locations of the EasyBuild configuration files and of the external modules     definition file.</p> </li> <li> <p>Settings for the module naming scheme: As we need to point to a custom implementation     of the module naming schemes, this is done through an environment variable. For     consistency we also set the module naming scheme itself via a variable and set     EASYBUILD_SUFFIX_MODULES_PATH as that together with the module naming scheme determines     the location of the modules with respect to the module install path.</p> </li> <li> <p>Custom EasyBlocks</p> </li> <li> <p>Search path for EasyConfig files with <code>eb -S</code> and <code>eb --search</code></p> <ol> <li> <p>Every directory of the robot search path is automatically included and      does not need to be added to EASYBUILD_SEARCH_PATHS</p> </li> <li> <p>The LUMI-EasyBuild-contrib repository,     as that one is not in the robot path in non-user mode.</p> </li> <li> <p>Not yet done, but we could maintain a local copy of the CSCS repository and     enable search in that also.</p> </li> <li> <p>Default EasyConfig files that come with EasyBuild (if we can find EasyBuild,     which is if an EasyBuild-build EasyBuild module is loaded)</p> </li> <li> <p>Deliberately not included: Our ebrepo_files repositories. Everything in there     should be in our own EasyConfig repository if the installations are managed     properly.</p> </li> </ol> </li> <li> <p>We also set containerpath and packagepath even though we don't plan to use those,     but it ensures that files produced by this option will not end up in our GitHub     repository.</p> </li> </ul>"},{"location":"easybuild_setup/#the-easybuild-user-mode","title":"The EasyBuild-user mode","text":"<ul> <li> <p>The root of the user EasyBuild directory structure is pointed to by the     environment variable <code>EBU_USER_PREFIX</code>. The default value if the variable     is not defined is <code>$HOME/EasyBuild</code>.</p> <p>Note that this environment variable is also used in the <code>LUMI/yy.mm</code> modules as these modules try to include the user modules in the MODULEPATH.</p> </li> <li> <p>The directory structure in that directory largely reflects the system     directory structure. This may be a bit more complicated than really needed     for the user who does an occasional install, but is great for user communities     who install a more elaborate software stack in their project directory.</p> <p>Changes:</p> <ul> <li> <p><code>SystemRepo</code> is named <code>UserRepo</code> instead and that name is fixed,      contrary to the <code>SytemRepo</code> name. We do keep it      as a separate level so that the user can also easily do version      tracking via a versioning system such as GitHub.</p> </li> <li> <p>The <code>mgmt</code> level is missing as we do not take into account      subdirectories that might be related to other software management      tools.</p> </li> <li> <p>As there are only modules generated by EasyBuild in this module tree,      <code>modules/easybuild</code> simply becomes <code>modules</code>.</p> </li> <li> <p>Similarly, the <code>EB</code> level in the directory for installed software is      omitted.</p> </li> </ul> </li> <li> <p>The robot search path:</p> <ol> <li> <p>The user repository for the currently active partition</p> </li> <li> <p>The user repository for the common partition (if different from     the previous one)</p> </li> <li> <p>The system repository for the currently active partition</p> </li> <li> <p>The system repository for the common partition (if different from     the previous one)</p> </li> <li> <p>The user EasyConfig directory <code>UserRepo</code> (even if it is not there     yet)</p> </li> <li> <p>The LUMI-EasyBuild-contrib repository, if present in the user      directory use that one and otherwise use the one from the central     installation.</p> </li> <li> <p>The LUMI-specific EasyConfig directory from the application directory</p> </li> </ol> </li> <li> <p>The search path for EasyConfig files with <code>eb -S</code> and <code>eb --search</code></p> <ol> <li> <p>The directories above in the robot search path are automatically also used     for search.</p> </li> <li> <p>Not yet done, but we could maintain a local copy of the CSCS repository and     enable search in that also.</p> </li> <li> <p>Default EasyConfig files that come with EasyBuild are deliberately not included     in user mode as it was decided this is confusing for the users.</p> </li> <li> <p>Deliberately not included: Our ebrepo_files repositories. Everything in there     should be in our own EasyConfig repository if the installations are managed     properly.</p> </li> </ol> <p>So currently the additional search paths in user mode are empty. </p> </li> </ul> <p>There are two regular configuration files:</p> <ol> <li> <p>The system <code>easybuild-production.cfg</code> is always read. In the current     implementation it is assumed to be present.</p> </li> <li> <p>The user <code>easybuild-user.cfg</code>(in <code>UserRepo/easybuild/config</code> in the user     direcgtory) is read next and meant for user-specific settings that should be     read for all LUMI software stacks.</p> </li> <li> <p>Then the system <code>easybuild-production-LUMI-yy.mm.cfg</code> is read after, hence can be used     to overwrite settings made in <code>production.cfg</code> for a specific toolchain. This     allows us to evolve the configuration files while keeping the possibility to install     in older versions of the LUMI software stack. This will overwrite generic     user options!</p> </li> <li> <p>Finally the user <code>easybuild-user-LUMI-yy.mm.cfg</code> is read for user     customizations to a specific toolchain.</p> </li> </ol> <p>Only the first of those 4 files has to be present. Presence of the others is detected when the module is loaded. Reload the module after creating one of these files to start using it.</p> <p>Comparison:</p> robot path non-user robot path user / ebrepo user active partition / ebrepo user common partition ebrepo system active partition ebrepo system active partition ebrepo system common partition ebrepo system common partition EBU_USER_PREFIX/UserRepo LUMI-EasyBuild-contrib user or system LUMI-SoftwareStack system LUMI-SoftwareStack system search path non-user search path user LUMI-EasyBuild-contrib system / Default easybuild EasyConfigs /"},{"location":"failed_experiments/","title":"Some failed experiments","text":"<p>These failed experiments are documented to avoid making the same mistake twice.</p>"},{"location":"failed_experiments/#overwriting-a-module-by-putting-one-with-the-same-name-and-version-earlier-in-the-path-may-not-work","title":"Overwriting a module by putting one with the same name and version earlier in the path may not work","text":"<p>During the development of the LUMI software stack prototype using the 21.04 CPE release on eiger it turned out that the <code>cpe/21.04</code> module had one nasty habbit: By setting <code>LMOD_MODULERCFILE</code> with the <code>setenv</code> LMOD function rather than by prepending or appending to it using <code>prependpath</code>/<code>append_path</code>, it cleared the <code>modulerc.lua</code> file that the LUMI software setup relies upon.</p> <p>We tried to cure that by putting a modified copy of the file in a directory earlier in the MODULEPATH and hiding the original Cray module with a <pre><code>hide_modulefile ( '/opt/cray/pe/lmod/modulefiles/core/cpe/21.04.lua' )\n</code></pre> line in the system <code>modulerc.lua</code> file. Though the module was indeed hidden as <code>module avail</code> confirmed, and though there was a <code>cpe/21.04.lua</code> file earlier in the MODULEPATH, <code>module load cpe/21.04</code> still loaded the wrong version, even with <code>LMOD_IGNORE_CACHE=1</code>.</p> <p>A clue to what is happening is that  when the line that hides the module in <code>modulerc.lua</code> is removed, you will notice that LMOD marks the one in the Cray <code>core/cpe</code> subdirectory as the default and not the one with the same name earlier in the MODULEPATH.</p> <p>The problem is that</p> <ol> <li> <p>When a full name is used, LMOD can still load a hidden module and does so when     that module is marked as the default.</p> </li> <li> <p>The CPE does mark one of the cpe modules as the default using a <code>.version</code> file     in the <code>core/cpe</code> subdirectory.</p> </li> </ol> <p>Luckily HPE-Cray uses a <code>.version</code> file which has the lowest priority. The solution is to mark a module in the overwrite directory as the default using either the symlink method or a <code>.modulerc.lua</code> file in that directory.</p>"},{"location":"failed_experiments/#a-change-of-visibility-of-modules-is-only-detected-when-the-module-command-exits","title":"A change of visibility of modules is only detected when the module command exits","text":"<p>The idea was to write a module <code>cpe/restore-defaults</code> to restore the defaults rather than calling a script. This didn't entirely work as expected, likely because LMOD only assesses visibility at the start of the execution. Whereas changing <code>MODULEPATH</code> triggers LMOD to reassess the module tree, making changes to <code>LMOD_MODULERCFILE</code> doesn't trigger reassessing the visibility.</p> <p>To recreate the experiment, take any standard <code>cpe</code> modulefile that doesn't only use those modules with the highest version number, e.g., one that uses an older version of gcc, ( and with the <code>setenv</code> replaced with <code>append_path</code> or <code>prepend_path</code> see the issues with the CPE), and the following <code>cpe/restore-defaults.lua</code> file: <pre><code>modules = {\n    \"PrgEnv-aocc\",\n    \"PrgEnv-cray\",\n    \"PrgEnv-gnu\",\n    \"PrgEnv-intel\",\n    \"PrgEnv-nvidia\",\n    \"aocc\",\n    \"atp\",\n    \"cce\",\n    \"cray-R\",\n    \"cray-ccdb\",\n    \"cray-cti\",\n    \"cray-dsmml\",\n    \"cray-fftw\",\n    \"cray-hdf5\",\n    \"cray-hdf5-parallel\",\n    \"cray-jemalloc\",\n    \"cray-libsci\",\n    \"cray-mpich\",\n    \"cray-netcdf\",\n    \"cray-netcdf-hdf5parallel\",\n    \"cray-openshmemx\",\n    \"cray-parallel-netcdf\",\n    \"cray-pmi\",\n    \"cray-pmi-lib\",\n    \"cray-python\",\n    \"cray-stat\",\n    \"craype\",\n    \"craype-dl-plugin-py3\",\n    \"craypkg-gen\",\n    \"gcc\",\n    \"gdb4hpc\",\n    \"iobuf\",\n    \"modules\",\n    \"nvidia\",\n    \"papi\",\n    \"perftools-base\",\n}\n\nif (mode() == \"load\" or mode() == \"show\") then\n    for _,mod in pairs(modules)\n    do\n        if (isloaded(mod)) then\n            unload(mod)\n            load(mod)\n        end\n    end\nend\n</code></pre> Then observe that <pre><code>module load cpe/21.04\nmodule load cpe/restore-defaults\n</code></pre> doesn't produce the same state as <pre><code>module load cpe/21.04\nmodule unload cpe/21.04\nmodule load cpe/restore-defaults\n</code></pre> even though loading <code>cpe/restore-defaults</code> triggers the unloading of <code>cpe/21.04</code>. In the former, lower versions of modules will remain loaded. However, loading <code>cpe/restore-defaults</code> a second time to correct the situation, i.e., <pre><code>module load cpe/21.04\nmodule load cpe/restore-defaults\nmodule load cpe/restore-defaults\n</code></pre> produces the expected result.</p>"},{"location":"files_used/","title":"An overview of files in the LUMI-SoftwareStack repository and where they are being used","text":"<p>As GitHub markdown has pretty poor tables we currently use a list layout.</p>"},{"location":"files_used/#craype-subdirectory","title":"CrayPE subdirectory","text":"<ul> <li> <p><code>CPEpackages_yy.mm.csv</code> files: Two-column .csv files defining the components     of the yy.mm CPE release.</p> <p>Much but not all of that data could be automatically extracted from the matching <code>/opt/cray/pe/cpe/yy.mm/modulerc.lua</code> file. See the procedure to install a new version of the Cray PE on the system.</p> </li> </ul>"},{"location":"files_used/#lmod-subdirectory","title":"LMOD subdirectory","text":"<ul> <li> <p>The <code>LMOD</code>      subdirectory itself is set as the value of the environment variable     <code>LMOD_PACKAGE_PATH</code> in the system setup.</p> </li> <li> <p><code>admin.list</code>: Set as the value of the environment variable      <code>LMOD_ADMIN_FILE</code>      in the system setup.</p> </li> <li> <p><code>lmodrc.lua</code>: Set as the value of the environment variable     <code>LMOD_RC</code> in the system setup.</p> </li> <li> <p><code>LUMIstack_yy.mm_modulerc.lua</code> files: Added to the list of modulerc files in     the <code>LMOD_MODULERCFILE</code> environment variable by the     <code>modules/LUMIstack</code> generic     module files for the <code>LUMI/yy.mm</code> modules.</p> <p>Sets the default versions for the CPE release yy.mm in a way that is independent of everything the <code>cpe/yy.mm</code> modules might do (unless they really overwrite <code>LMOD_MODULERCFILE</code>).</p> </li> <li> <p><code>LUMIstack_modulerc.lua</code> files: Added to the     list of modulerc files in the <code>LMOD_MODULERCFILE</code> environment variable by the     <code>modules/LUMIstack</code> generic     module files for the <code>LUMI/yy.mm</code> modules.</p> <p>Used to hide a number of modules that are of no use to ordinary users when using the LUMI software stacks, and provide some user-friendly aliases for the partition modules.</p> </li> <li> <p><code>SitePackage.lua</code>: Referred to indirectly by the     <code>LMOD_PACKAGE_PATH</code> system environment variable.</p> <p>Additional information on this file</p> </li> </ul>"},{"location":"files_used/#modules-subdirectory","title":"modules subdirectory","text":"<ul> <li> <p><code>CraEnv.lua</code>: Module providing the plain nearly unmodified     Cray Programming Environment.</p> <p>We do overwrite a number of CPE files in the <code>modules/CrayOverwrite</code> subdirectory to work around problems in some of the Cray module files.</p> </li> <li> <p><code>EasyBuild-config</code>: A generic EasyBuild     configuration module that instantiates into the <code>EasyBuild-production</code>,     <code>EasyBuild-infrastructure</code> and <code>EasyBuild-user</code>     modules in the relevant partitions of the LUMI software stacks.</p> </li> <li> <p><code>EasyBuild-unlock</code>: A module that has to be     loaded before any of the <code>EasyBuild-production</code> or <code>EasyBuild-infrastructure</code>     modules can be loaded as additional protection to not     accidentally overwrite a system installation.</p> </li> <li> <p><code>init-lumi</code>: The module called from the Cray PE intialisation     process (in <code>/etc/cray-pe.d/cray-pe-configuration.sh</code>) to do the final steps     of enabling the LUST software stacks.</p> </li> <li> <p><code>LUMIstack</code>: Generic implementation(s) of the LUMI software     stack modules. The actual modules can simply link to the right version of the generic     module file.</p> </li> <li> <p><code>LUMIpartition</code>: Generic implementation(s) of the     LUMI partition modules. The actual modules can simply link to the right version     of the generic module file.</p> </li> <li> <p><code>CrayOverwrite</code>: A directory with modulefiles that     should overwrite standard CPE module files that have behaviour that conflicts with     our setup.</p> <p>The process of overruling a Cray module may not be easy: If one of those modules is set as default in the original CPE subdirectory, one should make sure that that is overruled in the <code>CrayOverwrite</code> subdirectory for that module.</p> </li> <li> <p><code>StyleModifiers</code>: A set of small modules that change     the presentation of the module display by <code>module avail</code>. These modules simply     set one or more environment variables, one LUMI-specific one and the other LMOD     configuration variables.</p> <p>These modules include:</p> <ul> <li> <p><code>ModuleColour</code>: Switch between     colour and black-and-white display of the modules</p> </li> <li> <p><code>ModuleExtensions</code>: Show or     hide the display of extensions in the output of <code>module avail</code>.</p> </li> <li> <p><code>ModuleLabel</code>: Switch between three     ways of displaying the module subdirectories:</p> <ul> <li> <p><code>label</code>: Give the module subdirectories meaningful names and collapses     all directories in the CPE module hierarchy.</p> </li> <li> <p><code>PEhierarcy</code>: Give the module subdirectories meaningful names but do     not collapse the directories in the CPE module hierarchy</p> </li> <li> <p><code>system</code>: Show the directory names of all modules subdirectories</p> </li> </ul> </li> <li> <p><code>ModulePowerUser</code>: Enables the power     user view of the module system: Less modules are hidden, but using those modules     that are otherwise hidden is not supported by the LUMI User Support Team. These     modules may not work under a regular user account or are their use is not documented     in the regular LUMI documentation as they are only meant for support staff.</p> </li> <li> <p><code>ModuleStyle</code>: Used to return to     the situation at login or the default for the system.</p> </li> </ul> </li> </ul> <p>For the <code>EasyBuild-config</code>,  <code>LUMIstack</code> and <code>LUMIpartition</code> modules we adopted a special version numbering: They are numbered in the same way as CPE releases and a particular module for any LUMI stack we use the most recent version that is not younger than the correpsonding CPE/LUMI stack.</p>"},{"location":"files_used/#easybuildconfig-subdirectory","title":"easybuild/config subdirectory","text":"<p>The files are referred to by the <code>EasyBuild-config</code> generic module file.</p>"},{"location":"files_used/#easybuildeasyblocks-subdirectory","title":"easybuild/easyblocks subdirectory","text":"<p>This is the subdirectory for the LUMI custom EasyBlocks. We use a scheme with subdirectories as in the main EasyBlock repository.</p>"},{"location":"files_used/#easybuildeasyconfigs-subdirectory","title":"easybuild/easyconfigs subdirectory","text":"<p>EasyConfig repository for LUMI.</p>"},{"location":"files_used/#easybuildhooks-subdirectoruy","title":"easybuild/hooks subdirectoruy","text":"<p>This directory contains the custom hooks for EasyBuild on LUMI.</p> <p>The file names end on the first version of the software stack that they support. As such we can drop old code from hooks while still being able to regenerate old configirations.</p>"},{"location":"files_used/#easybuildtoolchains-subdirectoruy","title":"easybuild/toolchains subdirectoruy","text":"<p>This directory contains the custom Cray toolchain code for EasyBuild.</p> <p>For LUMI:</p> <ul> <li> <p>Custom toolchain definition files:</p> <ul> <li> <p><code>cpecray.py</code>: Toolchain based on the Cray compiler</p> </li> <li> <p><code>cpegnu.py</code>: Toolchain based on the GNU compilers provided by Cray</p> </li> <li> <p><code>cpeamd.py</code>:  Toolchain based on the AMD AOCC compiler provided by Cray.</p> </li> </ul> </li> <li> <p>Compiler definitions to use compilers through the Cray wrappers:</p> <ul> <li> <p><code>compiler/cpecce.py</code>: Definitions for the Cray CCE compiler with the Cray     wrappers.</p> </li> <li> <p><code>compiler/cpegcc.py</code>: Definitions for the GNU compilers with the Cray wrappers.</p> </li> <li> <p><code>compiler/cpeaocc.py</code>: Definitions for the AMD AOCC compilers with the Cray     wrappers.</p> </li> </ul> </li> </ul>"},{"location":"files_used/#easybuildtools-subdirectory","title":"easybuild/tools subdirectory","text":"<ul> <li> <p><code>module_naming_scheme</code> subdirectory:     LUMI uses a customised flat EasyBuild naming scheme. The links by moduleclass are     omitted as they are not used in our module system.</p> <p>Note that the <code>EasyBuild-config</code> generic module also sets the environment variable <code>EASYBUILD_SUFFIX_MODULES_PATH</code> to the empty string to omit the <code>all</code> level in the EasyBuild modules directory structure.</p> </li> </ul>"},{"location":"files_used/#the-etc-subdirectory","title":"The etc subdirectory","text":"<p>This directory contains the <code>motd.txt</code> and [<code>lumi_fortune.txt``](https://github.com/Lumi-supercomputer/LUMI-SoftwareStack/tree/main/etc/lumi_fortune.txt) files that are used by the</code>init-lumi` module to augment the message of the day and to display a random tip about LUMI.</p>"},{"location":"files_used/#the-scripts-and-scriptslumitools-subdirectories","title":"The scripts and scripts/lumitools subdirectories","text":"<p>The scripts subdirectory contains a number of shell scripts to initialise a new installation, new software stack in the installation, or various substeps of this. Several of those scripts are just wrapper scripts that call a Python routine to do the work.</p>"},{"location":"files_used/#the-setup-subdirectory","title":"The Setup subdirectory","text":"<p>A directory where we keep our preferred setup of the system configuration files for the HPE Cray PE, useful to communicate with the sysadmins who maintain those files.</p>"},{"location":"files_used/#the-tools-subdirectory","title":"The tools subdirectory","text":"<p>This directory contains scripts that are useful to any user of EasyBuild of LUMI, e.g., to bump EasyConfig files to a new release of the Cray PE.</p> <ul> <li><code>upgrade-tc.py</code>: Bump the version of the Cray PE in     an EasyConfig file, also adapting the name of the file. Note that the regular     dependencies are not updated to a new version.</li> </ul>"},{"location":"files_used/#the-testing-subdirectory","title":"The Testing subdirectory","text":"<p>This directory contains various files to be used in component tests.</p> <ul> <li><code>install_lmod_newest.sh</code> is a script to     install a very recent version of LMOD to test compatibility of the module system     agains more recent versions than the one that comes with a HPE-Cray system.</li> </ul>"},{"location":"files_used/#note-files-and-directories-referred-to-from-outside-the-lumi-software-structure","title":"Note: Files and directories referred to from outside the LUMI software structure","text":"<ul> <li> <p><code>LMOD</code> subdirectory itself as the value of the environment variable     <code>LMOD_PACKAGE_PATH</code></p> </li> <li> <p><code>LMOD/admin.list</code> as the value of the environment variable      <code>LMOD_ADMIN_FILE</code></p> </li> <li> <p><code>LMOD/lmodrc.lua</code> as the value of the environment variable     <code>LMOD_RC</code></p> </li> <li> <p>Not a reference to a file, but the system should set <code>LMOD_AVAIL_STYLE</code> to     <code>&lt;label&gt;:PEhierarchy:system</code>.</p> </li> </ul>"},{"location":"module_setup/","title":"Setup of the LUMI module system","text":""},{"location":"module_setup/#features-of-the-module-system-on-lumi","title":"Features of the module system on LUMI","text":"<ul> <li> <p>The module system is organised in software stacks enabled through loading a     meta-module. <code>CrayEnv</code> will enable the native Cray environment for developers     who prefer to do everything themselves, while the <code>LUMI/yy.mm</code> modules enable     the LUMI software stacks, based on the corresponding yy.mm version of the Cray     programming environment.</p> <p>To distinguish between versions of the software stack that are fairly complete and that we try to support for a longer period and those versions that are only meant to install a few applications that run into compiler bugs on the main versions, the latter versions of the software stack have a name of the form <code>LUMI/yy.mm.dev</code>. It is possible to at some point having both a <code>LUMI/yy.mm.dev</code> and <code>LUMI/yy.mm</code> software stack for the same release of the Cray PE so that if we figure out that a particular Cray PE release is good enough to build a new version of the software stack, we can do so without interupting users using the development version while still being able to update some components that are already installed in the .dev stack. However, they have to use the same components of the Cray PE and same cpeCray/cpeGNU/cpeAMD EasyBuild toolchain modules.</p> </li> <li> <p>When it matters, the module system will automatically select the software for     the partition on which the software stack was loaded (and change to the correct     partition in Slurm batch scripts if you reload the LUMI module). It is always possible     to overwrite by loading a partition module after (re)loading the software stack     meta-module.</p> <p>Moreover, we ensure as much as possible that after loading a different partition, already loaded modules are reloaded in as reasonable versions as possible.</p> <p>NOTE:</p> <ul> <li> <p>LMOD 8.3.x, the default on Cray with the 21.06 software release: <code>module update</code>     does not work correctly and the way to trigger a reload of the software stack     in case of a possible partition change is</p> <pre><code>module load LUMI/$LUMI_STACK_VERSION\n</code></pre> </li> <li> <p>In LMOD 8.4.x and 8.5.x, it is possible to instead use <code>module update</code>, and     there seems to be an undocumented <code>module reload</code> that has exactly the same     effect.</p> </li> </ul> </li> <li> <p>User-friendly presentation with clear labels is the default, and there is a user-friendly     way to change the presentation through modules rather than by setting LMOD environment     variables by hand.</p> <p>We also do an effort to hide modules that a regular user does not need at some point, e.g., EasyBuild configuration modules to install in the system directories, or those Cray PE modules that are not relevant for a particular version of the LUMI software stack.</p> </li> <li> <p>Discoverability of software is a key feature. The whole module system is designed     to make it easy to find installed software through <code>module spider</code> and <code>module     keyword</code> even if that software may not be visible at some point through <code>module     avail</code>.</p> </li> <li> <p>The modules that load the software stack and partition, and the modules that     influence the presentation of the modules are sticky so that <code>module purge</code>     will not remove them. Hence a user who wants to continue working in the same     software stack can use <code>module purge</code> to unload all software stack-specific     modules without having to start from scratch.</p> </li> </ul>"},{"location":"module_setup/#assumptions-in-the-implementation","title":"Assumptions in the implementation","text":"<ul> <li> <p>The SoftwareStack module relies on the <code>detect_LUMI_partition</code> function in     <code>SitePackage.lua</code> to determine on which partition it is running.</p> <p>The current implementation first checks if the environment variable LUMI_OVERWRITE_PARTITION is defined and if so, the value of that variable is used. It is assumed to be C, G, D or L depending on the node type (CPU compute, GPU compute, data and visualisation or login), or can be common to install software in the hidden common partition.</p> <p>If that environment variable is not defined, we currently emply the following algorithm as a demo of what can be done on the final LUMI installation:</p> <ul> <li> <p>On eiger uan01 and uan02 the partition is set to L</p> </li> <li> <p>On eiger uan03 the partition is set to common</p> </li> <li> <p>On all other hosts we first check for the environment variable     LUMI_PARTITION and use that one and otherwise we set the partition     to L.</p> </li> </ul> <p>This is used by the SoftwareStack module to then auto-load the module for the current partition when it is loaded. That function can be implemented differently also so that the environment variable is no longer needed.</p> </li> <li> <p>The current implementation assumes that <code>LMOD_PACKAGE_PATH</code> refers to the LMOD     directory in the repository as that is used to determine the name of the repository     (rather than use a hardcoded name, e./g/. <code>SystemRepo</code>).</p> </li> </ul>"},{"location":"module_setup/#default-behaviour-when-loading-modules","title":"Default behaviour when loading modules","text":"<ul> <li> <p>A user will always need to load a software stack module first.</p> <ul> <li>For those software stack modules that further split up according     to partition (the LUMI/yy.mm modules), the relevant partition     module will be loaded automatically when loading the software     stack. This is based on the partition detected by the     detect_LUMI_partition function defined in SitePackage.lua.</li> </ul> </li> <li> <p>We made the SoftwareStack and LUMIpartition      modules sticky so that once loaded a user can use <code>module purge</code> if they      want to continue working in the same software stack but load other packages.</p> </li> </ul>"},{"location":"module_setup/#module-presentation","title":"Module presentation","text":"<ul> <li> <p>The LUMI module system supports three ways of presenting the module to the users</p> <ol> <li> <p>The default way users user-friendly labels rather than subdirectories to present     the modules. Sometimes several subdirectories may map to the same label when     our feeling is that those modules belong together for the regular user.</p> <p>There are two variants of this view:</p> <p>a. All Cray PE module directories collapsed into a single category. This     style is called <code>label</code>.</p> <p>b. User-friendly names for the Cray PE module directories, but showing the     full hierarchy. This is called the <code>PEhierarchy</code> style.</p> </li> <li> <p>But of course the view with subdirectories is also supported for the power     user. This is called the <code>system</code> style.</p> </li> </ol> <p>The implementation of the labeled view is done in the <code>avail_hook</code> hook in the <code>SitePackage.lua</code> file and is derived from an example in the Lmod manual.</p> </li> <li> <p>To not confront the user directly with all Lmod environment variables that influence     the presentation of the modules, we also have a set of modules that can be used     to change the presentation, including</p> <ul> <li> <p>Switching between the label, PEhierarchy and system (subdirectory) view</p> </li> <li> <p>Showing extensions in module avail</p> </li> <li> <p>Turn on and off colour in the presentation of the modules as the colour selection     can never be good for both white background and black background (though of     course a power user can change the colour definitions in their terminal emulation     software to ensure that both bright and dark colours are displayed properly).</p> </li> </ul> <p>The presentation modules are sticky so that <code>module purge</code> doesn't change the presentation of the modules.</p> </li> <li> <p>There is a \"power user\" view that will reveal a number of modules that are     otherwise hidden. This is at their own risk. Use of this module is not documented     on the regular LUMI-documentation.</p> </li> <li> <p>There are also two <code>ModuleStyle</code> modules that try to reset the presentation to     the system defaults or whatever the user may have set through environment variables.</p> </li> </ul>"},{"location":"module_setup/#the-common-software-subdirectories","title":"The common software subdirectories","text":"<ul> <li> <p>We have a set of subdirectories for each of the 4 LUMI partitions on which the     modules are available. However, we also have a directory to install software that     is common to all partitions and doesn't need specific processor optimisations hence     is simply compiled for the minimal configuration that works everywhere, Rome without     accelerator.</p> </li> <li> <p>We use a hidden partition module (<code>partition/common</code>) which can then be recognized     by the EasyBuild configuration modules to install software in that partition.</p> <ul> <li> <p>Hiding is done via the <code>LMOD/LUMIstack_modulerc.lua</code> file.</p> </li> <li> <p>The path to the common software modules is added in the other partition module     files, always in such a way that the partition-specific directory comes before     the equivalent common directory to ensure that partition-specific software     has a higher priority.</p> <p>Yet because of the way that LMOD works, it is still better to either install a certain package only in the common partition or only in the other partitions as the order in the MODULEPATH is not always respected if a particular version is hard-marked as the default.</p> </li> <li> <p>Moreover, the labeling in SitePackage.lua is such that in the labeled view one     sees the common and partition-specific software is put together. This is another     reason to not have a package in both the common subdirectory and in one or     more of the regular partition subdirectories as it would appear twice in the     list for that partition.</p> </li> </ul> </li> <li> <p>A hidden partition module did show up into the output of <code>module spider</code> as a way to     reach certain other modules. This was the case even when it was hidden by using a name     starting with a dot rather than a modulerc.lua file. This \"feature\" is also mentioned     in Lmod issue #289.</p> <ul> <li> <p>Our solution was to modify the module file itself to not include the paths when the     mode is \"spider\" so that Lmod cannot see that it is a partition module that makes     other modules available.</p> <p>We currently do not show this for power users either which may be the best choice to avoid conflicts with caches.</p> </li> </ul> </li> <li> <p>We needed an additional module tree, <code>Infrastructure</code>, which works a pure hierarchy     (i.e., the common subdirectory is not loaded in the MODULEPATH of any other partition)     to ensure that <code>module update</code> and changing partitions works well for modules     that have to be present in all four regular partitions and the common meta-partition,     e.g., cpe* modules needed by EasyBuild and EasyBuild configuration modules.</p> </li> <li> <p>We also use a visibility hook for LMOD to hide modules from the Cray PE that are     not relevant for a particular version of the LUMI software stack.</p> <p>The data used by that hook is stored in the <code>mgmt/LMOD/VisibilityHookData</code> directory in a format that favours speed over readability and is auto-generated by a script based on the description of a version of the CPE.</p> </li> </ul>"},{"location":"module_setup/#the-crayenv-partition","title":"The CrayEnv partition","text":"<p>The <code>CrayEnv</code> partition is a hidden partition used to trigger cross-installation of software in the <code>CrayEnv</code> stack. This is done via a separate hidden partition as we need a way to avoid loading modules that will not be visible in that stack.</p> <p>This partition had only modules in the corresponding <code>Infrastructure</code> tree in the module system. To make EasyBuild available, we symlink in that directory to the EasyBuild subdirectory in the <code>common</code> partition of the same <code>LUMI/yy.mm</code> software stack.</p>"},{"location":"module_setup/#the-system-partition","title":"The system partition","text":"<p>The <code>system</code> partitions functions nearly identical to the <code>CrayEnv</code> partition but its function is to install software whose modules will be available system-wide, independent of any toolchain, and can be loaded as soon as the <code>init-lumi</code> module is loaded (which itself is loaded at login).</p>"},{"location":"module_setup/#where-do-we-set-the-default-modules","title":"Where do we set the default modules?","text":"<ul> <li> <p>Style modifiers: LMOD/modulerc.lua (central moduler.lua file)</p> </li> <li> <p>Software stack: Currently by a hidden <code>.modulerc.lua</code> file in the <code>SoftwareStacks</code>     subdirectory since we use different defaults on different test systems.</p> </li> <li> <p>Partition: No assigned default, the software stack module determines the optimal     partition based on the node where that module is loaded.</p> </li> <li> <p>Cray PE compoments: Depending on the version of the LUMI stack loaded, different     versions of the modules in the Cray PE will be the default ones, corresponding     to the release of the PE used for that stack. These defaults are set in     <code>LMOD/LUMIstack_&lt;version&gt;_modulerc.lua</code>, which can be generated with the     <code>make_CPE_modulerc.sh</code> script.</p> <p>Note that loading a <code>cpe/yy.mm</code> module may overwrite this depending on the implementation of that module.</p> </li> </ul>"},{"location":"module_setup/#helper-functions-in-sitepackagelua","title":"Helper functions in SitePackage.lua","text":"<p>See the separate SitePackage information file.</p>"},{"location":"procedures/","title":"Some procedures","text":""},{"location":"procedures/#installing-a-new-version-of-the-cray-pe-on-the-system","title":"Installing a new version of the Cray PE on the system","text":""},{"location":"procedures/#part-1-making-available-in-crayenv","title":"Part 1: Making available in CrayEnv","text":"<p>These instructions are enough to make the new CPE available in the <code>CrayEnv</code> stack but do not yet include instructions for EasyBuild</p> <ul> <li> <p>Install the CPE components using the instructions of HPE</p> </li> <li> <p>Create the components file in the <code>CrayPE</code> subdirectory of the repository.     This is currently a simple .csv-file but may be replaced by a different one if     HPE would provide that information in machine-readable format in a future version     of the CPE.</p> <p>Much of that information can be extracted from the HPE-Cray provided file <code>/opt/cray/pe/cpe/yy.mm/modulerc.lua</code>, though we do currently keep some additional packages in that file that don't have their defaults set in that file. One of them which is actually used in the LUMIpartition module file is the version of the craype-targets packages (essentially the CPE targeting modules).</p> </li> <li> <p>This step is only needed if you don't do part 2 as the installation script used     in part 2 of this procedure will also do this.</p> <p>Add a <code>cpe/yy.mm.lua</code> link for the just installed programming environment to the <code>modules/CrayOverwrite/core</code> subdirectory of the modules tree (NOT in the <code>modules</code> directory of the repository). This file does need the .csv file generated in the previous step to function properly.</p> <p>Note that this directory also contains a link named <code>.modulerc</code> to the <code>.version</code>file of `/opt/cray/pe/lmod/modulefiles/core/cpe''. It is essential that the default version of the <code>cpe</code> module is the same in the original module directory and in <code>CrayOVerwrite</code> as otherwise the original one may get loaded in some circumstances. By using the symbolic link and the new name with higher priority we assure that the modules from <code>CrayOVerwrite</code> are used. One still needs to ensure that the overwrite modules also are more to the front of the <code>MODULEPATH</code> or things still go wrong.</p> </li> <li> <p>This step is only needed if you don't do part 2 as the installation script used     in part 2 of this procedure will also do this.</p> <p>If <code>/opt/cray/pe/cpe/yy.mm/modulerc.lua</code> is missing on the system, an alternative needs to be generated and stored in <code>modules/CrayOverwrite/data-cpe/yy.mm</code> as it is used by the generic cpe module installed in the previous step.</p> <p>This can be done by running <code>./make_CPE_modulerc yy.mm</code> in the <code>scripts</code> subdirectory of <code>SystemRepo</code>.</p> </li> <li> <p>Hide the new cpe module installed in the Cray PE subdirectory by adding a line to     <code>LMOD/modulerc.lua</code> in the <code>LUMI-SoftwareStack</code> repository.</p> </li> <li> <p>If there are new Cray targeting modules that are irrelevant for LUMI you may want to     hide them in the respective code block in <code>LMOD/LUMIstack_modulerc.lua</code>     in the <code>LUMI-SoftwareStack</code> repository.</p> </li> </ul>"},{"location":"procedures/#part-2-adding-support-as-a-lumiyymmdev-software-stack","title":"Part 2: Adding support as a LUMI/yy.mm(.dev) software stack","text":"<p>From here on the setup is largely automated by the <code>prepare_LUMI_stack.sh</code> script in the <code>scripts</code> subdirectory of the repository. A few things need to be in place though before running this script:</p> <ul> <li> <p>The component definition of the Cray PE software stack in the .csv file in the     <code>CrayPE</code> subdirectory.</p> </li> <li> <p>An EasyConfig file for the right version of EasyBuild in the     <code>easybuild/easyconfigs/e/EasyBuild</code> subdirectory of the repository.</p> <p>Note that it is always possible to run <code>prepare_LUMI_stack.sh</code>-script with <code>EASYBUILD_IGNORE_CHECKSUMS=1</code> set if the checksums in the EasyConfig file are not yet OK.</p> </li> <li> <p>Add a software stack-specific configuration file     <code>easybuild-production-LUMI-yy.mm.cfg</code> for EasyBuild if     needed in the <code>easybuild/config</code> subdirectory of the repository.</p> <p>We currently don't include the <code>.dev</code> extension, i.e., if there is a development and an LTS software stack for the same version of the CPE, they share the EasyBuild configuration file. This makes sense because the development stack is meant to prepare for a production stack.</p> </li> </ul> <p>Furthermore you may want to make sure that the proper versions of the following files are available in the repository should you want to make changes compared to versions installed before:</p> <ul> <li> <p>The generic <code>EasyBuild-config</code> module should you want to make changes to, e.g.,     the environment variables et by that module.</p> </li> <li> <p>The generic <code>LUMIstack</code> and <code>LUMIpartition</code> module.</p> </li> <li> <p>The generic cpe module <code>cpe-generic</code>.</p> </li> </ul> <p>The software stack initialisation script will take the most recent one (based on the yy.mm version number) that is not newer than the release of the CPE for the software stack.</p> <p>The <code>prepare_LUMI_stack.sh</code> script takes three arguments, and the order is important:</p> <ol> <li> <p>The version of the software stack: the Cray PE version, with the extension <code>.dev</code>     for a development stack (e.g., 21.05.dev or 21.06).</p> </li> <li> <p>The version of EasyBuild to install in the software stack</p> </li> <li> <p>A work directory for temporary files, used to install a bootstrapping copy     of EasyBuild. Rather than trying to use an EasyBuild installation from an     older software stack if present to bootstrap the new one, we simply chose     to do a bootstrap every time the script is run as this is a procedure     that simply always works, though it is more time consuming.</p> <p>The advantage however is that one can just clone the production repository anywhere, run an initialisation script to initialise the structure around the repository, then initialise a software stack and start working.</p> </li> </ol> <p>The script then does the following steps:</p> <ul> <li> <p>Generate our own <code>cpe/yy.mm</code> module in <code>modules/CrayOverwrite/core</code> by creating     a symbolic link to the right version of the generic modules in <code>SystemRepo</code>.</p> </li> <li> <p>Check if the Cray PE comes with its own <code>modulerc.lua</code> file with the default components     (if so, that file can be found in <code>/opt/cray/pe/cpe/yy.mm</code>). If not an alternative     file is generated from the data in the compoments .csv file and stored in     <code>modules/CrayOverwrite/data-cpe/yy.mm</code>.</p> <p>The file is used by the generic <code>cpe/yy.mm</code> module in <code>modules/CrayOVerwrite/core</code>.</p> </li> <li> <p>Create the software stack module files by symlinking to the generic implementations     in <code>SystemRepo</code>.</p> </li> <li> <p>Create the partition modules (second level in the hierarchy) by symlinking to the     generic implementations in <code>SystemRepo</code>.</p> </li> <li> <p>Create the <code>LUMIstack_yy.mm_modulerc.lua</code> file in <code>mgmt/LMOD/ModuleRC</code>.     Currently this file only contain references to Cray PE modules and as such correspond     to the <code>modulerc.lua</code> file in <code>/opt/cray/pe/cpe/yy.mm</code> but his may change in     a future version. Whereas the aforementioned <code>modulerc.lua</code> files are meant to     be activated by the <code>cpe.yy.mm</code> modules and hence to be used by any software     stack that uses the Cray PE, the <code>LUMIstack_yy.mm_modulerc.lua</code> are really meant     exclusively for the LUMI software stack.</p> <p>This is done by running the <code>make_LUMIstack_modulerc.sh</code> script.</p> </li> <li> <p>Creates the <code>CPE_modules_*.lua</code> file in <code>mgmt/LMOD/VisibilityHookData</code> for     <code>module avail</code> visibility hook in <code>SitePackage.lua</code>. This is done by running     the <code>make_CPE_VisibilityHookData.sh</code> script.</p> </li> <li> <p>Creates the full directory structure for the software stack for the modules,     the binary installations and the EasyBuild repo.</p> </li> <li> <p>Creates the EasyBuild external modules definition file from the data in the     corresponding <code>CPEpakcages_&lt;CPE version&gt;.csv</code> file (if the file does not yet     exist). This file is stored in the repo as it may be useful for other people     who check out the repository to know what is going on.</p> </li> <li> <p>Creates the EasyBuild-production, EasyBuild-infrastructure and EasyBuild-user     modules for each partition by softlinking to the matching generic file in     the <code>SystemRepo</code>.</p> </li> <li> <p>Downloads the selected version of EasyBuild to the EasyBuild sources directory     if the files are not yet present.</p> </li> <li> <p>Installs a bootstrapping version of EasyBuild in the work directory. As that     version will only be used to install EasyBuild from our own EasyConfig file,     there is no need to also install the EasyConfig files.</p> </li> <li> <p>Loads the software stack module for the common partition and the EasyBuild-production     module to install EasyBuild in the software stack.</p> </li> <li> <p>Create EasyConfig files in the repository for the <code>cpeCray</code>, <code>cpeGNU</code> and <code>cpeAMD</code>     toolchains (if those files do not yet exist) and use the just installed EasyBuild     to install them in the <code>Infrastructure</code> module subdirectory for all 4 regular     and the common partition.</p> <p>The toolchain definition EasyConfig files are generated by the <code>make_CPE_EBfile.sh</code> script.</p> </li> </ul> <p>Things to do afterwards:</p> <ul> <li>If you want to change the default version of the LUMI software stack module,     you can do this by editing <code>.modulerc.lua</code> in     <code>modules/SoftwareStack/LUMI</code>.</li> </ul> <p>And now one should be ready to install other software...</p> <ul> <li> <p>Ensure the software stack module is loaded and the partition module for the     location where you want to install the software is loaded (so the hidden module     partition/common to install software in the location common to all regular     partitions)</p> </li> <li> <p>Load EasyBuild-production for an install in the production directories or     EasyBuild-user for an install in the user or project directories (and you'll     need to set some environment variables beforehand to point to that location).</p> </li> </ul>"},{"location":"procedures/#making-a-master-installation-of-the-software-stack","title":"Making a master installation of the software stack","text":"<p>A master version is an installation of the full system that can run independently from another installation. This is made easy so that it is possible to install a master version under a personal account or on a test system for testing and development independent from the central installation on the system.</p> <ol> <li> <p>Create an empty directory which will be the root of the software installation.</p> </li> <li> <p>Clone the <code>LUMI-SoftwareStack</code> repository in that directory. You can change     the name of the repository directory to anything as the whole installation is independent     of that name provided that <code>LMOD_PACKAGE_PATH</code> refers to the LMOD installation in that     repository. In the documentation, we call the directory <code>SystemRepo</code> (as opposed     to <code>UserRepo</code> for a user installation that depends on a master installation).</p> </li> <li> <p>Run the <code>prepare_LUMI.sh</code> script from <code>SystemRepo/scripts</code> to initialise the     directory structure. The script takes no arguments but it is important to run the     version in the <code>SytemRepo</code> in the software installation that needs to be initialised.</p> </li> <li> <p>Running the <code>enable_LUMI.sh</code> script from <code>SystemRepo/scripts</code> prints the (bash)     shell commands that need to be executed to initialise the shell to start using this     software installation. A very good way is to use it with <code>eval</code>:     <pre><code>eval $(&lt;directory&gt;/SystemRepo/scripts/enable_LUMI.sh)\n</code></pre></p> </li> </ol> <p>Now use the procedure \"Installing a new version of the Cray PE on the system\" to start a new version of the LUMI software stack, skipping the creation of those files that may already be in the repository because someone else has done them already.</p>"},{"location":"procedures/#bumping-the-toolchain-to-a-newer-version-without-changing-the-other-dependencies","title":"Bumping the toolchain to a newer version without changing the other dependencies","text":"<p>For now, we use the script <code>upgrade-tc.py</code> developed at CSCS and kept in the <code>tools</code> subdirectory. It is also available when any of our EasyBuild configuration modules is loaded.</p>"},{"location":"procedures/#bumping-the-toolchain-to-a-newer-version-and-changing-the-other-dependencies","title":"Bumping the toolchain to a newer version and changing the other dependencies","text":"<p>Two scripts situated in the <code>tools</code> subdirectory and available in the path when any of the EasyBuild configuration modules is loaded can help with this:</p> <ul> <li><code>upgrade-tc.py</code> is a script developed at CSCS to upgrade the toolchain in the EasyConfig     file. It will also update the name of the EasyConfig file to reflect the new toolchain version.</li> <li><code>upgrade-locals.txt</code> uses the set of <code>local_*_version</code> variable defintions in one of the     <code>versions-yy.mm.txt</code> files to update those variables in one or more given EasyConfig file.</li> </ul>"},{"location":"start_on_LUMI_pilot/","title":"Setting up the stack on LUMI","text":"<p>There are currently two options to use the LUST LUMI software stacks</p> <ol> <li> <p>There is already one installed in <code>/appl/lumi</code> but not yet activated     (as of December 14). However, it requires very little to activate.</p> </li> <li> <p>You can still follow the procedure for the initial pilot projects, when     nothing was installed already. Take a look at the \"Installing the stack     from scratch\" section.</p> </li> </ol> <p>Augmenting the stack with software installed in <code>/projappl</code> follows the same procedure in both cases.</p>"},{"location":"start_on_LUMI_pilot/#using-the-pre-installed-stack","title":"Using the pre-installed stack","text":"<p>Simply run <code>/appl/lumi/LUMI-SoftwareStack/activate_LMOD_LUMI.sh</code> and it will tell you what to do. Note that those instructions change over time as the stack is still in beta and evolves, so if you run into problems it is good to check them again.</p>"},{"location":"start_on_LUMI_pilot/#installing-the-stack-from-scratch","title":"Installing the stack from scratch","text":"<ul> <li> <p>Create the directory where you want to install the software stack and move into that directory:</p> <pre><code>mkdir LUMI-pilot\ncd LUMI-pilot\n</code></pre> <p>If you want to share the installation with everybody in your project, doing so in your <code>/projappl</code> directory may be a good choice. However, beware as a full installation will eat into your file quota. Parallel file systems are really meant to work with large files while a software installation unfortunately can contain many small files.</p> <p>You can also work in scratch but the problem there is that the installation will be removed automatically after 90 days.</p> </li> <li> <p>Clone the LUMI-SoftwareStack and LUMI-EasyBuild-contrib repositories:</p> <p><pre><code>git clone git@github.com:Lumi-supercomputer/LUMI-SoftwareStack.git\ngit clone git@github.com:Lumi-supercomputer/LUMI-EasyBuild-contrib.git\n</code></pre> or <pre><code>git clone https://github.com/Lumi-supercomputer/LUMI-SoftwareStack.git\ngit clone https://github.com/Lumi-supercomputer/LUMI-EasyBuild-contrib.git\n</code></pre> depending on whether you have ssh access to GitHub or not.</p> <p>git clone  git@github.com:klust/LUMI-SoftwareStack.git</p> </li> <li> <p>Move into the <code>LUMI-SoftwareStack/scripts</code> subdirectory</p> <pre><code>cd LUMI-SoftwareStack/scripts\n</code></pre> </li> <li> <p>Now we will first build the raw structure of the software stack:</p> <pre><code>./prepare_LUMI.sh\n</code></pre> </li> <li> <p>Since currently CPE 21.05 is installed on LUMI, but we have no intention     to use it in a LUMI software stack, we run a script to simply make CPE 21.05     available in the CrayEnv stack of our system.</p> <pre><code>./prepare_CrayEnv_stack.sh 21.05\n</code></pre> </li> <li> <p>Now is time to prepare the LUMI/21.08 stack for which we used EasyBuild 4.4.1.     Some work space is also needed but can be erased afterwards. Though not ideal,     $HOME/work is used in the command as this variable is available to all users.</p> <pre><code>./prepare_LUMI_stack.sh 21.08 4.4.2 $HOME/work\n</code></pre> <p>This step does take a while though.</p> </li> <li> <p>Now we first need to activate LMOD and point it to the right module subdirectories.     This is done by a script which is in the LUMI-SoftwareStack subdirectory itself     (as it is only a temporary solution). The instructions to do this are generated     by running the <code>activate_LMOD_LUMI.sh</code> script in the LUMI-SoftwareStack     directory:</p> <pre><code>cd ..\n./activate_LMOD_LUMI.sh\n</code></pre> <p>You can call this script at any time to regenerate the instructions as long as you call it from within the repository so that it can detect the directory.</p> <p>Follow the instructions. We suggest that for now you simply copy the 5 lines of code as indicated in the text.</p> <p>Now we are ready to begin installing software.</p> </li> <li> <p>Assuming you are still in the <code>LUMI-pilot/LUMI-SoftwareStack</code> directory:     We'll use some lists of software that are in a subdirectory:</p> <pre><code>cd easybuild/easystacks/LUMI-21.08\n</code></pre> </li> <li> <p>If you want some elementary build tools etc to be available in the <code>CrayEnv</code>     environment, which is really just the plain Cray Programming Environment with     minimal interference from the LUMI software stacks, you can generate those     using:</p> <pre><code>module --force purge\nml LUMI/21.08 partition/CrayEnv\nml EasyBuild-CrayEnv\neb --experimental --easystack  production-CrayEnv-21.08.yaml\n</code></pre> </li> <li> <p>The regular software stack consists of multiple parts:</p> <ul> <li> <p>A number of building blocks that are not performance-critical and have been     installed with the system compilers.</p> </li> <li> <p>Complete lines of software compiled with either <code>PrgEnv-gnu</code>, <code>PrgEnv-cray</code> or <code>PrgEnv-aocc</code>.     However, in EasyBuild these toolchains are loaded by loading respectively the     <code>cpeGNU/21.08</code>, <code>cpeCray/21.08</code> or <code>cpeAMD/21.08</code> toolchains.</p> <p>At the moment of writing, <code>cpeAMD/21.-08</code> does not work as the modules are installed on the system by Cray but the software is missing.</p> </li> </ul> <p>So this is the moment to consider what you want to use. The common building blocks are always needed though, though some users may not need the <code>Rust</code> module which will also be installed with the procedure below. However, if you only intend to use, e.g., the GNU compilers, it makes no sense to install all <code>cpeCray</code> or <code>cpeAMD</code> modules.</p> </li> <li> <p>To install the common part of building blocks:</p> <pre><code>module --force purge\nml LUMI/21.08 partition/common\nml EasyBuild-production\neb --experimental --easystack  production-common-21.08.yaml\n</code></pre> </li> <li> <p>To install the cpeGNU modules:</p> <pre><code>module --force purge\nml LUMI/21.08 partition/L\nml EasyBuild-production\neb -r --experimental --easystack  production-L-21.08-GNU.yaml\n</code></pre> <p>Note the <code>-r</code> argument that was absent before. This is because due to problems with the so-called EasyStack files (those yaml files) it was not yet possible to specify all required EasyBuild recipes, and recursive installations are disabled by default.</p> </li> <li> <p>To install the cpeCraymodules:</p> <pre><code>module --force purge\nml LUMI/21.08 partition/L\nml EasyBuild-production\neb -r --experimental --easystack  production-L-21.08-Cray.yaml\n</code></pre> <p>Note again the <code>-r</code> argument for the very same reasons.</p> </li> </ul> <pre><code>mkdir LUMI-pilot\ncd LUMI-pilot\ngit clone  git@github.com:klust/LUMI-SoftwareStack.git\ngit clone git@github.com:Lumi-supercomputer/LUMI-EasyBuild-contrib.git\ncd LUMI-SoftwareStack/scripts\n./prepare_LUMI.sh\n./prepare_CrayEnv_stack.sh 21.05\n./prepare_LUMI_stack.sh 21.08 4.4.2 $HOME/work\n</code></pre>"},{"location":"start_on_LUMI_pilot/#installing-additional-software","title":"Installing additional software","text":"<ul> <li> <p>If you installed the full stack in your own project directory, you can actually     have two locations to install software with EasyBuild</p> <ul> <li> <p>In the main stack (which you will of course not be able to do once we have     a central installation): Use EasyBuild-production.</p> </li> <li> <p>It is however also possible to have your own personal stack. By default this     will be build in your home directory, but it might be better to share with     your colleagues in your project.</p> <p>To use any other location then the default one, every user of the stack should ensure that the environment variable <code>EBU_USER_PREFIX</code> points to the directory. After that, the LUMI software stack will take care of everything and activate the right modules. However, you now have to use the module EasyBuild-user for the installation.</p> </li> </ul> </li> <li> <p>We recommend to have your own repository in <code>$EBU_USER_PREFIX</code> where you have     all EasyConfig files that you want to use. The mandatory name of that repository     directory is <code>UserRepo</code>(as otherwise it is not found by EasyBuild-user), and     EasyConfig files should be in <code>UserRepo/easybuild/easyconfigs</code>.</p> <p>For more complete and technical information on our setup, please read the other pages pointed to in the README document in this docs directory.</p> </li> <li> <p>Note that we cannot support the common EasyBuild toolchains on LUMI in this phase     as for now we concentrate on the Cray Programming Environment for which we have     support from HPE-Cray. This does mean that EasyBuild recipes have to be reworked.</p> </li> <li> <p>We're still working on reducing module clutter on the system. For now all basic     libraries have their own module. Some of those may be hidden in the future as few     users use them directly. However, even then they can still be found with     <code>module spider</code> and be loaded.</p> <p>We have a few special modules on LUMI that bundle a lot of software that is otherwise in separate modules on a typical EasyBuild installation.</p> <ul> <li> <p><code>buildtools/21.08</code>(and other respective versions for each LUMI/yy.mm stack)     bundles a number of popular build tools. This is done in a single module to     have them all available in a single command and to try to use a very consistent     set of build tools throughout most of our software installation process.</p> <p>Try <code>module help buildtools/21.08</code> to check what's in it.</p> </li> <li> <p><code>systools/15.1.0</code> contains some useful tools, such as <code>tree</code> and <code>htop</code>.</p> </li> <li> <p><code>syslibs/15.1.0</code> is a hidden module and is really just meant as a build dependency     for some basic tools that we install.</p> </li> </ul> </li> </ul>"},{"location":"whats_new/","title":"What's new or different?","text":"<ul> <li> <p>2023-06-19: <code>module avail</code> now also shows the hugepages modules     and the relevant target modules for the partition.</p> </li> <li> <p>2023-06-16: EasyBuild now clears the user cache to force a      regeneration of the cache.</p> </li> <li> <p>2023-04-11: Various improvements to the generation of the files     that contain information for Lmod about the CPE and the visibility     hook.</p> </li> <li> <p>Several updates missing...</p> </li> <li> <p>2022-04-22: Corrections to the EasyBuild-config modules to avoid     having twice the same directory in the output of searches.</p> </li> <li> <p>2022-03-01: Not every non-development stack should be marked as a     LTS stack as it turns out to be very difficult to guarantee longevity     of the software stack. Hence there is now a table at the top of     SitePackage.lua to mark which stacks are LTS.</p> </li> <li> <p>2022-03-01: Option to turn off the tip in the message-of-the-day or     even the whole message-of-the-day.</p> </li> <li> <p>2022-02-24: Hiding cpe-cuda modules via LMOD/modulerc.lua as they don't     make sense on LUMI.</p> </li> <li> <p>2022-02-23: Added support for a system partition with software that is     available everywhere, EasyBuild-CrayEnv has been replaced with     EasyBuild-production.</p> </li> <li> <p>2021-12-15: Moved the list of default modules to load out of CrayEnv to     SitePackage.lua so and now also load target modules in the LUMIpartition     module so that users who want to use PrgEnv instead of cpeGNU/cpeCray/cpeAMD     get the right target modules.</p> </li> <li> <p>2021-12-14: Added init-lumi, a module to finish the initialisation that is     called from the Cray PE initialisation process. The module also adds to     the message-of-the-day and adds a fortune-like tip about using LUMI.</p> </li> <li> <p>2021-12-14: Removed adding the overwrite CPE modules to the MODULEPATH in     CrayEnv and LUMI (actually the partition modules) as at system rollout it     seemed they created as much problems as they solved.</p> </li> <li> <p>2021-10-26: Implemented a dummy partition <code>CrayEnv</code> in the LUMI software     stack to cross-install to CrayEnv to offer a simple way to install additional     tools using the SYSTEM toolchain only.</p> </li> <li> <p>2021-10-19: Removed the <code>EB</code> level in the directory hierarchy for binaries     in the EasyBuild-user installation as the whole tree is only meant for     EasyBuild-installed software.</p> </li> <li> <p>2021-09-21: Added a new toolchain option to cpeGNU: <code>gfortran9-compat</code> that     adds compatibility flags to <code>FFLAGS</code>, <code>FCFLAGS</code> and <code>F90FLAGS</code> to improve     compatibility with gfortran 9. Currently this is only     <code>-fallow-argument-mismatch</code>.</p> </li> <li> <p>2021-09-14: Loading a LUMI software stack module now also sets the environment     variable LUMI_STACK_CPE_VERSION which can be used to know which version of the     CPE the software stack is for (useful if stack is a development stack with     name ending on .dev)</p> </li> <li> <p>2021-09-14: Added the <code>tools</code> subdirectory with scripts that we want to make     available in the PATH and that are useful for any user, not just for a one-time     setup of the repository or adding a new software stack to the repository (the     latter remain in the <code>scripts</code> subdirectory and are not put in the PATH when     loading one of our EasyBuild configuration modules).</p> </li> <li> <p>2021-08-11: Added the LUMI-EasyBuild-contrib     repository and made it part of the search path (but not of the robot path)     of the EasyBuild-* modules.</p> </li> <li> <p>2021-08-03: Changed the SitePackage.lua function that detects the LUMI partition,     and this has influence on how the repository should be used for testing.</p> <ul> <li> <p>The environment variable LUMI_OVERWRITE_PARTITION is now used to overwrite     any automatic selection of the partition.</p> </li> <li> <p>A demo selection process based on the hostname was implemented:</p> <ul> <li> <p>On eiger uan01 and uan02 the partition is set to L</p> </li> <li> <p>On eiger uan03 the partition is set to common</p> </li> <li> <p>On all other hosts we first check for the environment variable     LUMI_PARTITION and use that one and otherwise we set the partition     to L.</p> </li> </ul> </li> </ul> <p>The <code>enable_LUMI.sh</code> script now sets <code>LUMI_OVERWRITE_PARTITION</code> rather than <code>LUMI_PARTITION</code> so if you use that script to set the environment, you shouldn't note anything.</p> </li> </ul>"},{"location":"whats_new/#toolchain-setup-compared-to-the-cscs-setup-of-june-2021","title":"Toolchain setup compared to the CSCS setup of June 2021","text":"<ul> <li> <p>The CSCS definition for <code>cpeAMD</code> relied on a definition of the     <code>aocc</code> compilers that used GNU compiler options that likely did not always work     and couldn't be found in the AOCC manuals. They have been replaced with Clang-based     options.</p> </li> <li> <p>The single implementation for all <code>cpe*</code> compilers in <code>compilers/cpe.py</code> has     been replaced by a different implementation for each compiler. The reasons are     that</p> <ul> <li> <p>Not all data that should be copied from the non-Cray versions of the compiler     to configure the compiler was actually copied.</p> </li> <li> <p>We found bugs in the implementation of the GNU compilers in EasyBuild and by     using a separate file could work around them until we now for sure they are     bugs and what the original intent was.</p> </li> <li> <p>Some compiler options may not map onto a flag but may need further processing,     and having separate files makes that a bit easier until we have enough insight     in all problems that may occur to bundle them again in a single implementation.</p> </li> </ul> </li> <li> <p>Improvements to the Cray compiler file:</p> <ul> <li>The Cray compiler wrappers provide <code>-openmp</code> and <code>-noopenmp</code> flags to     turn OpenMP on or off, and these should work with all supported compilers.     These were implemented to solve the problem that for the CCE compilers     with the Classic Cray Fortran compiler but new clang-based C compiler     the options for enabling OpenMP are different which is something that     is not appreciated by EasyBuild.</li> </ul> </li> </ul>"},{"location":"Toolchains/cpeAMD/","title":"cpeAMD toolchain","text":"<p>Note: The options are for the <code>aocc.py</code> file included in this repository and are not the same as those for the repository at CSCS.</p>"},{"location":"Toolchains/cpeAMD/#note-about-the-compilers","title":"Note about the compilers","text":"<ul> <li>AMD/ROCm 5.0.2 is based on LLVM 14.0.0</li> </ul>"},{"location":"Toolchains/cpeAMD/#available-options","title":"Available options","text":"<p>The cpeAMD toolchain supports the common toolchain options, the additional AMD flags and some additional Cray-specific flags, two of which are really just redefinitions of standard compiler flags.</p>"},{"location":"Toolchains/cpeAMD/#default-toolchain-options-that-may-not-behave-as-expected","title":"Default toolchain options that may not behave as expected","text":"<ul> <li> <p><code>rpath</code>: rpath linking in EasyBuild has not been properly tested in the HPE Cray PE.</p> </li> <li> <p><code>unroll</code>: As it is not clear from the docunentation for the Fortran compiler which     options should be set to enable or disable (if any at all) </p> </li> </ul>"},{"location":"Toolchains/cpeAMD/#amd-specific-toolchain-options","title":"AMD-specific toolchain options","text":"Option Categorie What? lto code generation Enable Link Time Optimization (in the default 'full' mode) offload-lto code generation Enable LTO for offload compilation (in the default 'full' mode)"},{"location":"Toolchains/cpeAMD/#cpeamd-specific-flags-coming-from-the-hpe-cray-pe-compiler-wrappers","title":"cpeAMD-specific flags (coming from the HPE Cray PE compiler wrappers)","text":"Option Categorie What? dynamic code generation Generate dynamically linked executable (default: True) mpich-mt parallelism Alternate Cray-MPICH library for MT support (default: False) <p>mpich-mt: Directs the driver to link in an alternate version of the Cray-MPICH library which provides fine-grained multi-threading support to applications that perform MPI operations within threaded regions. (default: False)</p> <p>Two further options trigger different compiler flags than in the GCC toolchain: <code>verbose</code> and <code>optarch</code> but have otherwise the same meaning.</p>"},{"location":"Toolchains/cpeAMD/#mapping-of-options-onto-compiler-flags","title":"Mapping of options onto compiler flags","text":""},{"location":"Toolchains/cpeAMD/#compiler-optimization-level","title":"Compiler optimization level","text":"<p>The common options translate into:</p> Option Flag noopt -O0 lowopt -O1 defaultopt -O2 -fvectorize -fslp-vectorize opt -O3 <p>Other optimization-related options (and see also parallelism below):</p> Option Flag unroll -funroll-loops optarch TODO"},{"location":"Toolchains/cpeAMD/#floating-point-precision","title":"Floating point precision","text":"<p>The decision of our mapping is based partly on information from Cray and partly on looking through various manuals on Clang.</p> <ul> <li>Level <code>strict</code>: We simply stick to <code>-ffp-model=strict</code>. This automatically disables     all of the <code>-ffast-math</code> enablements so there is no need to add <code>-fno-fast-math</code>.     Note that at this level, fused multiply adds are disabled.</li> <li>Level <code>precise</code>: We stick to <code>-ffp-model=precise</code> and add <code>-ffp-contract=fast-honor-pragmas</code>.</li> <li>Default level: Currently the same as <code>strict</code>.</li> <li>Level <code>loose</code>: Set to <code>-ffp-model=fast</code> but try to turn on again a few safeguards:     <code>-ffp-contract=fast-honor-pragmas</code>, <code>-fhonor-infinities</code>, <code>-fhonor-nans</code>,     <code>-fsigned-zeros</code>.</li> <li>Level <code>veryloose</code>: Set to <code>-ffp-model=fast</code></li> </ul> <p>Note: Very recnet versions of clang add <code>-ffp-contract=fast-honor-pragmas</code> which may be interesting to add to <code>precise</code>, <code>defaultprec</code> and <code>loose</code> but is not yet supported by AOCC 2.2.</p> Option Flag strict -ffp-model=strict precise -ffp-model=precise defaultprec -ffp-model=precise loose -ffp-model=fast -fhonor-infinities -fhonor-nans -fsigned-zeros veryloose -ffp-model=fast <p>Other floating-point optimisation and accuracy-related flags:</p> Option What? ieee Not supported in clang/flang"},{"location":"Toolchains/cpeAMD/#common-parallelism-related-options","title":"Common parallelism-related options","text":"Option Flag vectorize False: -fno-vectorize -fno-slp-vectorize True: -fvectorize -fslp-vectorize loop -ftree-switch-conversion -floop-interchange -floop-strip-mine -floop-block openmp -fopenmp usempi No compiler flags mpich-mt -craympich-mt"},{"location":"Toolchains/cpeAMD/#code-generation-and-linking-options","title":"Code generation and linking options","text":"Option Flag dynamic No flag as this is currently the only mode supported lto -flto offload-lto -foffload-lto 32bit -m32 debug -g pic -fPIC packed-linker-options Pack the linker options as comma separated list (default: False) shared -shared static -static rpath Use RPATH wrappers when --rpath is enabled in EasyBuild configuration (default: True) <p>NOTE: <code>rpath</code> has not yet been checked and is likely broken. There is a better way to do this in the /hPE Cray PE  than what EasyBuild does as the wrappers already support rpath linking.</p>"},{"location":"Toolchains/cpeAMD/#source-related-options","title":"Source-related options","text":"Option Flag cstd -std=%(value)s i8 -fdefault-integer-8 r8 -fdefault-real-8 f2c -ff2c"},{"location":"Toolchains/cpeAMD/#miscellaneous-options","title":"Miscellaneous options","text":"Option Flag verbose -craype-verbose"},{"location":"Toolchains/cpeAOCC/","title":"cpeAOCC toolchain","text":"<p>Note: The options are for the <code>aocc.py</code> file included in this repository and are not the same as those for the repository at CSCS.</p>"},{"location":"Toolchains/cpeAOCC/#note-about-the-compilers","title":"Note about the compilers","text":"<p>From the user guides, introduction section:</p> <ul> <li>AOCC 2.1 is based on LLVM 9.0 release (llvm.org, 19th Sep 2019) with improved     Flang Fortran frond-end added with F2008 features and bug fixes.</li> <li>AOCC 2.2 is based on LLVM 10.0 release (llvm.org, 24th Mar 2020) with improved     Flang Fortran front-end added with F2008 features and bug fixes.</li> <li>AOCC 3.0     is based on LLVM 12 trunk (llvm.org, 22nd Oct 2020) with Flang as a Fortran front-end     added with F2008, Real 128 features. AOCC 3.0 also includes the support for OpenMP Debugging     Interface (OMPD) APIs.</li> <li>AOCC 3.1      release is based on LLVM 12 release (llvm.org, 14th April 2021) with Flang as a     Fortran front-end added with F2008 and Real 128 features. It is an incremental version of AOCC     3.0 that includes bug fixes and a support for compiler directives in Flang.</li> <li>AOCC 3.2     AOCC 3.2 is based on the LLVMTM 13 compiler infrastructure (llvm.org, 4 October 2021) and     includes bug fixes and support for other new features.</li> </ul>"},{"location":"Toolchains/cpeAOCC/#available-options","title":"Available options","text":"<p>The cpeAMD toolchain supports the common toolchain options, the additional AOCC flags and some additional Cray-specific flags, two of which are really just redefinitions of standard compiler flags.</p>"},{"location":"Toolchains/cpeAOCC/#aocc-specific-flags","title":"AOCC-specific flags","text":"<p>The following options map on AOCC-dpecific compiler flags can be similar to similar options in  the cpeGNU toolchain:</p> Option Categorie What? lto code generation Enable Link Time Optimization"},{"location":"Toolchains/cpeAOCC/#cpeaocc-specific-flags","title":"cpeAOCC-specific flags","text":"Option Categorie What? dynamic code generation Generate dynamically linked executable (default: True) mpich-mt parallelism Alternate Cray-MPICH library for MT support (default: False) <p>mpich-mt: Directs the driver to link in an alternate version of the Cray-MPICH library which provides fine-grained multi-threading support to applications that perform MPI operations within threaded regions. (default: False)</p> <p>Two further options trigger different compiler flags than in the GCC toolchain: <code>verbose</code> and <code>optarch</code> but have otherwise the same meaning.</p>"},{"location":"Toolchains/cpeAOCC/#mapping-of-options-onto-compiler-flags","title":"Mapping of options onto compiler flags","text":""},{"location":"Toolchains/cpeAOCC/#compiler-optimization-level","title":"Compiler optimization level","text":"<p>The common options translate into:</p> Option Flag noopt -O0 lowopt -O1 defaultopt -O2 -fvectorize -fslp-vectorize opt -O3 <p>Other optimization-related options (and see also parallelism below):</p> Option Flag unroll -funroll-loops optarch TODO"},{"location":"Toolchains/cpeAOCC/#floating-point-precision","title":"Floating point precision","text":"<p>The decision of our mapping is based partly on information from Cray and partly on looking through various manuals on Clang.</p> <ul> <li>Level <code>strict</code>: We simply stick to <code>-ffp-model=strict</code>. This automatically disables     all of the <code>-ffast-math</code> enablements so there is no need to add <code>-fno-fast-math</code>.     Note that at this level, fused multiply adds are disabled.</li> <li>Level <code>precise</code>: We stick to <code>-ffp-model=precise</code> and add <code>-ffp-contract=fast-honor-pragmas</code>.</li> <li>Default level: Currently the same as <code>strict</code>.</li> <li>Level <code>loose</code>: Set to <code>-ffp-model=fast</code> but try to turn on again a few safeguards:     <code>-ffp-contract=fast-honor-pragmas</code>, <code>-fhonor-infinities</code>, <code>-fhonor-nans</code>,     <code>-fsigned-zeros</code>.</li> <li>Level <code>veryloose</code>: Set to <code>-ffp-model=fast</code></li> </ul> <p>Note: Very recnet versions of clang add <code>-ffp-contract=fast-honor-pragmas</code> which may be interesting to add to <code>precise</code>, <code>defaultprec</code> and <code>loose</code> but is not yet supported by AOCC 2.2.</p> Option Flag strict -ffp-model=strict precise -ffp-model=precise defaultprec -ffp-model=precise loose -ffp-model=fast -fhonor-infinities -fhonor-nans -fsigned-zeros veryloose -ffp-model=fast <p>Other floating-point optimisation and accuracy-related flags:</p> Option What? ieee Not supported in clang/flang"},{"location":"Toolchains/cpeAOCC/#common-parallelism-related-options","title":"Common parallelism-related options","text":"Option Flag vectorize False: -fno-vectorize -fno-slp-vectorize True: -fvectorize -fslp-vectorize loop -ftree-switch-conversion -floop-interchange -floop-strip-mine -floop-block openmp -fopenmp usempi No compiler flags mpich-mt -craympich-mt"},{"location":"Toolchains/cpeAOCC/#code-generation-and-linking-options","title":"Code generation and linking options","text":"Option Flag dynamic No flag as this is currently the only mode supported lto -flto 32bit -m32 debug -g pic -fPIC packed-linker-options Pack the linker options as comma separated list (default: False) shared -shared static -static rpath Use RPATH wrappers when --rpath is enabled in EasyBuild configuration (default: True)"},{"location":"Toolchains/cpeAOCC/#source-related-options","title":"Source-related options","text":"Option Flag cstd -std=%(value)s i8 -fdefault-integer-8 r8 -fdefault-real-8 f2c -ff2c"},{"location":"Toolchains/cpeAOCC/#miscellaneous-options","title":"Miscellaneous options","text":"Option Flag verbose -craype-verbose"},{"location":"Toolchains/cpeCray/","title":"cpeCray toolchain","text":"<p>NOTE: Stuff in italics has to be checked: Do these options really work?</p>"},{"location":"Toolchains/cpeCray/#note-about-the-compilers","title":"Note about the compilers","text":"<ul> <li>The major version of the <code>cce</code> module corresponds to the major version of     the clang/LLVM compilers on which it is based for all versions that are     relevant for LUMI.</li> </ul>"},{"location":"Toolchains/cpeCray/#available-options","title":"Available options","text":"<p>The cpeCray toolchain supports the common toolchain options, and some additional Cray-specific flags, two of which are really just redefinitions of standard compiler flags.</p>"},{"location":"Toolchains/cpeCray/#compiler-flags-that-are-inherited-from-the-llvm-compilers-on-which-the-compilers-are-based","title":"Compiler flags that are inherited from the LLVM compilers on which the compilers are based","text":"Option Categorie What? lto code generation Enable Link Time Optimization (in the default 'full' mode) offload-lto code generation Enable LTO for offload compilation (in the default 'full' mode) <p>These options should not be used in projects that also contains Fortran code.</p>"},{"location":"Toolchains/cpeCray/#cpecray-specific-flags","title":"cpeCray-specific flags","text":"Option Categorie What? dynamic code generation Generate dynamically linked executable (default: True) mpich-mt parallelism Alternate Cray-MPICH library for MT support (default: False) <p>mpich-mt: Directs the driver to link in an alternate version of the Cray-MPICH library which provides fine-grained multi-threading support to applications that perform MPI operations within threaded regions. (default: False)</p> <p>Two further options trigger different compiler flags than in the GCC toolchain: <code>verbose</code> and <code>optarch</code> but have otherwise the same meaning.</p>"},{"location":"Toolchains/cpeCray/#mapping-of-options-onto-compiler-flags","title":"Mapping of options onto compiler flags","text":""},{"location":"Toolchains/cpeCray/#some-cray-specific-remarks","title":"Some Cray-specific remarks","text":"<ul> <li> <p>A number of options are different for the clang-based C/C++ compilers and Fortran     compiler, something that EasyBuild cannot easily deal with. This is, e.g., the     case for the options that set the floating point computation model, so for now     these are left blank.</p> <ul> <li> <p>Fortran floating point optimisation and accuracy options: <code>-hfp0</code> to <code>-hfp4</code>,     with <code>-hfp2</code> the default according to the manual.</p> <p>The manual only claims <code>-hfp0</code> to <code>-hfp3</code>.</p> </li> <li> <p>For OpenMP the manual also claims a different option for the Fortran compiler,     but it turns out that recent versions also support <code>-fopenmp</code> so no changes     are needed with respect to the standard EasyBuild behaviour.</p> <p>The manual claims that there is also a <code>-fnoopenmp</code> but that turns out to be wrong in cce 12.</p> </li> </ul> </li> <li> <p>The situation about promoting integer and real in Fortran to 8 bytes is very confusing.     Those options are not properly documented in the manuals. The Cray driver manual     only mentions <code>-default64</code> which according to the documentation is then converted     to <code>-sdefault64</code>. In the documentation, there is mention of <code>-sreal64</code> (or     <code>-s real64`), but this option is nowhere to be found in the list of compiler options.     However, it turns out that</code>-sinteger64<code>(or</code>-s integer64<code>) and</code>-sreal64<code>(or</code>-s real64``) both exist and do what is expected from them.</p> </li> </ul> <p>TODO: Check if -flto exist in Fortran and C and if so, add to the options.</p>"},{"location":"Toolchains/cpeCray/#compiler-optimization-level","title":"Compiler optimization level","text":"<p>The common options translate into:</p> Option Flag noopt -O0 lowopt -O1 defaultopt -O2 opt -O3 <p>Other optimization-related options (and see also parallelism below):</p> Option Flag unroll -funroll optarch TODO"},{"location":"Toolchains/cpeCray/#floating-point-precision","title":"Floating point precision","text":"<p>These flags are currently not correctly honoured.</p> Option Flag strict / precise / defaultprec / loose / veryloose / <p>Other floating-point optimisation and accuracy-related flags:</p> Option What? ieee /"},{"location":"Toolchains/cpeCray/#common-parallelism-related-options","title":"Common parallelism-related options","text":"Option Flag vectorize / openmp -fopenmp usempi No compiler flags mpich-mt -craympich-mt"},{"location":"Toolchains/cpeCray/#code-generation-and-linking-options","title":"Code generation and linking options","text":"Option Flag dynamic No flag as this is currently the only mode supported 32bit -m32 debug -g pic -fPIC packed-linker-options Pack the linker options as comma separated list (default: False) shared -shared static -static rpath Use RPATH wrappers when --rpath is enabled in EasyBuild configuration (default: True)"},{"location":"Toolchains/cpeCray/#source-related-options","title":"Source-related options","text":"Option Flag cstd -std=%(value)s i8 -sinteger64 r8 -sreal64"},{"location":"Toolchains/cpeCray/#miscellaneous-options","title":"Miscellaneous options","text":"Option Flag verbose -craype-verbose"},{"location":"Toolchains/cpeGNU/","title":"cpeGNU toolchain","text":""},{"location":"Toolchains/cpeGNU/#available-options","title":"Available options","text":"<p>The cpeGNU toolchain supports the common toolchain options, the additional GCC flags and some additional Cray-specific flags, two of which are really just redefinitions of standard compiler flags.</p>"},{"location":"Toolchains/cpeGNU/#gcc-specific-flags","title":"GCC-specific flags","text":"Option Categorie What? loop parallelism Automatic loop parallellisation f2c source Generate code compatible with f2c and f77 lto code generation Enable Link Time Optimization"},{"location":"Toolchains/cpeGNU/#cpegnu-specific-flags","title":"cpeGNU-specific flags","text":"Option Categorie What? dynamic code generation Generate dynamically linked executable (default: True) mpich-mt parallelism Alternate Cray-MPICH library for MT support (default: False) gfortran9-compat source Add flags that improve compatibility with gfortran 9 for 10 and higher (default: False) <p>mpich-mt: Directs the driver to link in an alternate version of the Cray-MPICH library which provides fine-grained multi-threading support to applications that perform MPI operations within threaded regions. (default: False)</p> <p>Two further options trigger different compiler flags than in the GCC toolchain: <code>verbose</code> and <code>optarch</code> but have otherwise the same meaning.</p>"},{"location":"Toolchains/cpeGNU/#mapping-of-options-onto-compiler-flags","title":"Mapping of options onto compiler flags","text":""},{"location":"Toolchains/cpeGNU/#compiler-optimization-level","title":"Compiler optimization level","text":"<p>The common options translate into:</p> Option Flag noopt -O0 lowopt -O1 defaultopt -O2 -ftree-vectorize opt -O3 <p>Other optimization-related options (and see also parallelism below):</p> Option Flag unroll -funroll-loops optarch TODO"},{"location":"Toolchains/cpeGNU/#floating-point-precision","title":"Floating point precision","text":"Option Flag strict -mieee-fp -mno-recip precise -mno-recip defaultprec -fno-math-errno loose -fno-math-errno -mrecip -mno-ieee-fp veryloose -fno-math-errno -mrecip=all -mno-ieee-fp <p>Other floating-point optimisation and accuracy-related flags:</p> Option What? ieee -mieee-fp -fno-trapping-math"},{"location":"Toolchains/cpeGNU/#common-parallelism-related-options","title":"Common parallelism-related options","text":"Option Flag vectorize False: -fno-tree-vectorize True: -ftree-vectorize loop -ftree-switch-conversion -floop-interchange -floop-strip-mine -floop-block openmp -fopenmp usempi No compiler flags mpich-mt -craympich-mt"},{"location":"Toolchains/cpeGNU/#code-generation-and-linking-options","title":"Code generation and linking options","text":"Option Flag dynamic No flag as this is currently the only mode supported 32bit -m32 debug -g pic -fPIC packed-linker-options Pack the linker options as comma separated list (default: False) shared -shared static -static rpath Use RPATH wrappers when --rpath is enabled in EasyBuild configuration (default: True)"},{"location":"Toolchains/cpeGNU/#source-related-options","title":"Source-related options","text":"Option Flag cstd -std=%(value)s i8 -fdefault-integer-8 r8 -fdefault-real-8 f2c -ff2c"},{"location":"Toolchains/cpeGNU/#miscellaneous-options","title":"Miscellaneous options","text":"Option Flag verbose -craype-verbose"},{"location":"Toolchains/toolchain_common/","title":"Common toolchain options","text":""},{"location":"Toolchains/toolchain_common/#optimization-level","title":"Optimization level","text":"<p>EasyBuild distinguishes between four optimization levels. Rather than having a single toolchain option that takes the level as a number, <code>toolchainopts</code> uses four boolean parameters where one takes precedence over others. Lower optimization takes precedence over higher optimization. All have as default value <code>False</code> yet <code>defaultopt</code> is the one that will be used if nothing is specified.</p> Option What? noopt Disable compiler optimizations lowopt Low compiler optimizations defaultopt Default compiler optimizations opt High compiler optimizations <p>Other optimization-related options (and see also parallelism below):</p> Option What? unroll Unroll loops (default: False) optarch Enable architecture optimizations (default: False)"},{"location":"Toolchains/toolchain_common/#floating-point-accuracy","title":"Floating point accuracy","text":"<p>There are five flags that set the floating point precision. All are <code>False</code> by default but <code>defaultprec</code> is taken if none of the options is set to <code>True</code>. Again, the first one that is set to <code>True</code> in the table below is used:</p> Option What? strict Strict (highest) precision precise High precision defaultprec Default precision loose Loose precision veryloose Very loose precision <p>Other floating-point optimisation and accuracy-related flags:</p> Option What? ieee Adhere to IEEE-754 rules (default: False)"},{"location":"Toolchains/toolchain_common/#common-parallelism-related-options","title":"Common parallelism-related options","text":"Option What? vectorize Enable compiler auto-vectorization, default except for noopt and lowopt openmp Enable OpenMP (default: False) usempi Use MPI compiler as default compiler (default: False) <p>The <code>usempi</code> option is only supported by toolchains that also include an MPI component.</p>"},{"location":"Toolchains/toolchain_common/#code-generation-and-linking-options","title":"Code generation and linking options","text":"Option What? 32bit Compile 32bit target (default: False) debug Enable debug (default: False) pic Use PIC (default: False) packed-linker-options No flag, internal to EasyBuild shared Build shared library (default: False) static Build static library (default: False) rpath No flag, internal to EasyBuild"},{"location":"Toolchains/toolchain_common/#source-related-options","title":"Source-related options","text":"Option What? cstd Specify C standard (C/C++ only - default: None) i8 Integers are 8 byte integers (Fortran only - default: False) r8 Real is 8 byte real (Fortran only - default: False)"},{"location":"Toolchains/toolchain_common/#miscellaneous-options","title":"Miscellaneous options","text":"Option What? verbose Verbose output (default: False) cciscxx Use CC as CXX (default: False) extra_cflags Specify extra CFLAGS options. (default: None) extra_cxxflags Specify extra CXXFLAGS options. (default: None) extra_f90flags Specify extra F90FLAGS options. (default: None) extra_fcflags Specify extra FCFLAGS options. (default: None) extra_fflags Specify extra FFLAGS options. (default: None) <p>Most of these options do not directly transform into compiler flags. Instead, they influence the way EasyBuild sets variables (<code>cciscxx</code>)  or directly add additional flags to the indicated environment variables.</p>"},{"location":"Toolchains/toolchain_cpe_common/","title":"EasyBuild CPE toolchains common options","text":""},{"location":"Toolchains/toolchain_cpe_common/#general-principles","title":"General principles","text":"<p>The toolchains for the CPE in this repository have been developed from those of CSCS. Several changes have been made though.</p> <ul> <li> <p>The AMD AOCC toolchain was very incomplete: It relied on a definition for the AOCC     compiler that was derived from GCC rather than from the Clang one, so most of the     options didn't really work.</p> <p>We also changed the floating point accuracy options compared to those used in the Clang compiler definition as they produced warnings about combinations that don't make sense.</p> </li> <li> <p>Some corrections were made to the Cray CCE compiler definition. In particular we     added the correct options to support the <code>-i8</code> and <code>-r8</code> toolchain options     for Fortran data types.</p> <p>We also had a look at better options for the floating point accuracy but there we ran into the problem that the options for the Cray compiler are different for the Fortran and C/C++ compilers, something that the option mapping mechanism in EasyBuild cannot deal with.</p> <p>For OpenMP there was an easy solution as it turns out that the Fortran compiler now does support <code>-fopenmp</code> as an alternative for <code>-homp</code> so there a single option that works for both the Fortran and new Clang-based C/C++ compilers is available.</p> </li> <li> <p>The code for processing <code>--optarch</code>/<code>EASYBUILD_OPTARCH</code> was extended to be more     in line with how other toolchain definitions process that code, and to prepare for     the GPUs where we may want to load multiple targeting modules (one for the CPU and     one for the GPU, or even one for the network). It is now possible to specify the     argument to <code>optarch</code> for multiple compilers. For the Cray toolchains, the name     <code>CPE</code> is used. This makes it possible to also support more traditional EasyBuild     toolchains simultaneously from the same setup should the need arrise.</p> <p>We did stick to the approach of loading the targeting modules rather than using the compiler flags <code>-target-cpu</code>, <code>-target-accel</code> and <code>-target-network</code> as now the implementation doesn't need to figure out which module is of which type (though that is very easy to do).</p> </li> </ul> <p>The CSCS implementation used a common setup to all CPE compilers. For LUMI, this path is left for now as it turned out the code was incomplete and it was difficult to complete the code and to get it working in all circumstances. Not all default options of the regular GNU and AOCC compilers were picked up by their corresponding CPE compiler definitions. Moreover, the generic approach does not work with toolchain options that do not simply map onto compiler options. This may change in a future edition again when it becomes clearer which code is really common to all Cray compiler definitions and which code is specialised for a particular compiler.</p> <p>In the process we also ran into several bugs in the EasyBuild toolchain definitions that define toolchain options that are never picked up and translated into compiler flags because the compiler definition fails to add them to <code>COMPILER_C_UNIQUE_FLAGS</code>/<code>COMPILER_F_UNIQUE_FLAGS</code> (likely the recommended way) or to push them onto the list <code>COMPILER_FLAGS</code> defined in the <code>compiler.py</code> generic compiler definition from which all other compiler definitions inherit.</p>"},{"location":"Toolchains/toolchain_cpe_common/#specifying-the-targets-through-optarcheasybuld_optarch","title":"Specifying the targets through --optarch/EASYBULD_OPTARCH","text":"<p>The actual architecture arguments are specified through the names of the corresponding targeting modules but dropping the <code>craype-</code> prefix from their name. Multiple options can be specified by using a <code>+</code>-sign between two options. However, only one CPT target, one accelerator target and one network target should be specified as otherwise the second will overwrite the first one anyway. If you want to specify options for multiple compilers, you can use the regular EasyBuild syntax for that, labeling the options for the Cray toolchains with <code>CPE:</code>.</p> <p>The <code>GENERIC</code> target is currently not supported for the Cray compilers because it is not clear how to do that in a system-independent way on Cray systems. There may be modules that define a generic architecture, but if they exist they are certainly not installed on all Cray systems.</p> <p>Some examples are</p> <ul> <li><code>EASYBUILD_OPTARCH=\"x86-rome\"</code>: Defines only an architecture for the Cray compilers,     in this case it tells EasyBuild to optimize for the AMD Rome processor.</li> <li><code>EASYBUILD_OPTARCH=\"x86-milan+accel-AMD-gfx90A+network-ofi\"</code>: Tells EasyBuild to     target for the AMD Milan CPU, AMD gfx90A GPU (which is the likely module name for     the MI 200) and OFI network stack.</li> <li><code>EASYBULD_OPTARCH=\"CPE:x86-rome;Intel:march=core-avx2 -mtune=core-avx2;GCC:march=znver2 -mtune=znver2\"</code>     would set a compiler target for the Cray toolchains, the Intel compilers used through the     regular EasyBuild common toolchains and the GNU compilers, again used through     the regular EasyBuild common toolchains, in each case specifying options suitable     for the AMD Rome CPU.</li> </ul>"}]}